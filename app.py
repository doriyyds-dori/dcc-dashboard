import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
import os
import traceback
from datetime import datetime

# --- Page Config ---
st.set_page_config(page_title="Audi DCC æ•ˆèƒ½çœ‹æ¿", layout="wide", page_icon="ğŸï¸")

# --- CSS Styling ---
# è°ƒæ•´ selectbox æ ·å¼ï¼Œä½¿å…¶åœ¨å¹¶æ’æ—¶æ›´ç¾è§‚
st.markdown(
    """
    <style>
        .top-container {display: flex; align-items: center; justify-content: space-between; padding-bottom: 20px; border-bottom: 2px solid #f0f0f0;}
        .metric-card {background-color: #fff; border: 1px solid #e0e0e0; border-radius: 8px; padding: 15px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);}
        /* è°ƒæ•´ä¸‹æ‹‰æ¡†æ ·å¼ï¼Œé˜²æ­¢è¿‡å®½æˆ–è¿‡çª„ */
        div[data-testid="stSelectbox"] {width: 100%;} 
        .big-font {font-size: 18px !important; font-weight: bold;}
    </style>
    """,
    unsafe_allow_html=True,
)

# --- Constants & Config ---
ADMIN_PASSWORD = "AudiSARR3"
DATA_DIR = "data_store"
os.makedirs(DATA_DIR, exist_ok=True)

# Fixed filenames (Operational Data)
PATH_F = os.path.join(DATA_DIR, "funnel.xlsx")
PATH_D = os.path.join(DATA_DIR, "dcc.xlsx")
PATH_A = os.path.join(DATA_DIR, "ams.xlsx")
PATH_S_XLSX = os.path.join(DATA_DIR, "store_rank.xlsx")
PATH_S_CSV = os.path.join(DATA_DIR, "store_rank.csv")

# Fixed filenames (Master Data)
PATH_M = os.path.join(DATA_DIR, "store_mapping.xlsx")  # ä»£ç†å•†å½’å±è¡¨

LAST_UPDATE_FILE = os.path.join(DATA_DIR, "_last_upload_time.txt")


# --- Helper Functions ---

def save_uploaded_file(uploaded_file, save_path: str) -> bool:
    try:
        with open(save_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        return True
    except Exception as e:
        st.error(f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
        return False


def get_store_rank_path():
    if os.path.exists(PATH_S_XLSX):
        return PATH_S_XLSX
    if os.path.exists(PATH_S_CSV):
        return PATH_S_CSV
    return None


def get_data_update_time(store_rank_path: str | None):
    """è¿”å›æœ€æ–°ä¸€æ¬¡ä¸Šä¼ æ•°æ®æŠ¥çš„æ—¶é—´"""
    if os.path.exists(LAST_UPDATE_FILE):
        try:
            with open(LAST_UPDATE_FILE, "r", encoding="utf-8") as f:
                txt = f.read().strip()
            if txt:
                return datetime.fromisoformat(txt)
        except Exception:
            pass

    paths = [PATH_F, PATH_D, PATH_A]
    if store_rank_path:
        paths.append(store_rank_path)

    mtimes = []
    for p in paths:
        if p and os.path.exists(p):
            try:
                mtimes.append(os.path.getmtime(p))
            except Exception:
                pass

    if not mtimes:
        return None

    ts = max(mtimes)
    return datetime.fromtimestamp(ts)


def dedupe_columns(columns):
    """æŠŠé‡å¤åˆ—åå˜æˆ: åˆ—å, åˆ—å__1, åˆ—å__2"""
    seen = {}
    out = []
    for c in list(columns):
        c = str(c)
        if c not in seen:
            seen[c] = 0
            out.append(c)
        else:
            seen[c] += 1
            out.append(f"{c}__{seen[c]}")
    return out


def smart_read(file_path: str, is_rank_file: bool = False):
    """é²æ£’è¯»å–ï¼ˆxlsx/csv/è¯¯åç¼€ xlsxï¼‰+ è‡ªåŠ¨æ‰¾è¡¨å¤´ + åˆ—åå»é‡"""
    if not file_path or not os.path.exists(file_path):
        return None

    df = None

    # å°è¯•ç›´æ¥è¯»å– Excel
    try:
        with open(file_path, "rb") as f:
            sig = f.read(4)
        if sig == b"PK\x03\x04" or sig.startswith(b"PK"): # Excel signature check
            df = pd.read_excel(file_path, header=None)
    except Exception:
        pass

    if df is None:
        is_csv = str(file_path).lower().endswith((".csv", ".txt"))
        encodings = ["utf-8-sig", "gb18030", "utf-16", "gbk"]
        for enc in encodings:
            try:
                df = pd.read_csv(file_path, header=None, encoding=enc, engine="python", on_bad_lines="skip")
                break
            except (UnicodeDecodeError, pd.errors.ParserError):
                continue
            except Exception:
                continue

    if df is None or df.empty:
        return None

    # è¿™é‡Œçš„ keywords å¢åŠ å½’å±è¡¨ç›¸å…³çš„åˆ—åï¼Œé˜²æ­¢æ‰¾é”™è¡¨å¤´
    keywords = ["é—¨åº—", "é¡¾é—®", "ç®¡å®¶", "æ’å", "ä»£ç†å•†", "åºå·", "çº¿ç´¢", "è´¨æ£€", "æ·»åŠ å¾®ä¿¡", "åŒºåŸŸç»ç†", "çœä»½", "åŸå¸‚"]
    header_row = 0

    search_rows = 20 if is_rank_file else 15
    for i in range(min(search_rows, len(df))):
        row_values = df.iloc[i].astype(str).str.cat(sep=",")
        if any(k in row_values for k in keywords):
            header_row = i
            break

    df.columns = df.iloc[header_row]
    df = df[header_row + 1:].reset_index(drop=True)

    df.columns = (
        df.columns.astype(str)
        .str.strip()
        .str.replace("\n", "", regex=False)
        .str.replace("\r", "", regex=False)
    )

    df.columns = dedupe_columns(df.columns)

    df = df.loc[:, df.columns.notna()]
    df = df.loc[:, df.columns != "nan"]

    return df


def clean_percent_col(df: pd.DataFrame, col_name: str):
    if col_name not in df.columns:
        return
    series = df[col_name].astype(str).str.strip().str.replace("%", "", regex=False)
    numeric_series = pd.to_numeric(series, errors="coerce").fillna(0)
    if numeric_series.max() > 1.0:
        df[col_name] = numeric_series / 100
    else:
        df[col_name] = numeric_series


def safe_div(df: pd.DataFrame, num_col: str, denom_col: str):
    if num_col not in df.columns or denom_col not in df.columns:
        return pd.Series([0] * len(df))
    num = pd.to_numeric(df[num_col], errors="coerce").fillna(0)
    denom = pd.to_numeric(df[denom_col], errors="coerce").fillna(0)
    result = (num / denom).replace([np.inf, -np.inf], 0).fillna(0)
    return result


def _to_1d_numeric(x):
    """æŠŠ Series æˆ–DataFrame å‹æˆ 1 åˆ—æ•°å€¼ Series"""
    if isinstance(x, pd.DataFrame):
        tmp = x.apply(pd.to_numeric, errors="coerce")
        return tmp.bfill(axis=1).iloc[:, 0].fillna(0)
    return pd.to_numeric(x, errors="coerce").fillna(0)


def _pick_col_exact(df: pd.DataFrame, exact_name: str):
    """ç²¾ç¡®æŸ¥æ‰¾åˆ—å"""
    for c in df.columns:
        if str(c).strip() == exact_name:
            return c
    return None

def _pick_any_col(df: pd.DataFrame, any_keywords, exclude_keywords=None):
    """æ¨¡ç³ŠæŸ¥æ‰¾åˆ—å"""
    exclude_keywords = exclude_keywords or []
    for c in df.columns:
        s = str(c)
        if any(k in s for k in any_keywords) and not any(x in s for x in exclude_keywords):
            return c
    return None

# --- Data Processing ---

@st.cache_data(ttl=300)
def process_data(path_f, path_d, path_a, path_s, path_m):
    try:
        # è¯»å–ä¸šåŠ¡æ•°æ®
        raw_f = smart_read(path_f)
        raw_d = smart_read(path_d)
        raw_a = smart_read(path_a)
        raw_s = smart_read(path_s, is_rank_file=True)
        
        # è¯»å–å½’å±å…³ç³»æ•°æ® (Master Data)
        raw_m = smart_read(path_m)

        if raw_f is None or raw_d is None or raw_a is None or raw_s is None:
            return None, None

        # ==========================================
        # 0. å‡†å¤‡å½’å±æ˜ å°„è¡¨ (Store Mapping)
        # ==========================================
        df_mapping = None
        def strict_clean_str(series):
            return series.astype(str).str.strip().str.replace(r'\s+', '', regex=True).str.lower().replace('nan', '')

        if raw_m is not None:
            # æ ‡å‡†åŒ–åˆ—åï¼Œé˜²æ­¢Excelé‡Œçš„åˆ—åæœ‰ç©ºæ ¼
            raw_m = raw_m.rename(columns=lambda x: str(x).strip())
            
            # è¯†åˆ«å…³é”®åˆ—
            col_mgr = _pick_any_col(raw_m, ["åŒºåŸŸç»ç†", "å¤§åŒºç»ç†"])
            col_prov = _pick_any_col(raw_m, ["çœä»½", "çœ"])
            col_city = _pick_any_col(raw_m, ["åŸå¸‚", "å¸‚"])
            col_store = _pick_any_col(raw_m, ["é—¨åº—åç§°", "ä»£ç†å•†", "ç»é”€å•†"])

            if col_mgr and col_store:
                df_mapping = raw_m[[col_store]].copy()
                df_mapping.rename(columns={col_store: "é—¨åº—åç§°"}, inplace=True)
                
                df_mapping["åŒºåŸŸç»ç†"] = raw_m[col_mgr] if col_mgr else "æœªçŸ¥"
                df_mapping["çœä»½"] = raw_m[col_prov] if col_prov else "æœªçŸ¥"
                df_mapping["åŸå¸‚"] = raw_m[col_city] if col_city else "æœªçŸ¥"
                
                # æ¸…æ´—é—¨åº—åä½œä¸º Key
                df_mapping["Join_Key"] = strict_clean_str(df_mapping["é—¨åº—åç§°"])
                # å»é‡ï¼Œé˜²æ­¢ä¸€å¯¹å¤š
                df_mapping = df_mapping.drop_duplicates(subset=["Join_Key"])

        # ==========================================
        # 1. å¤„ç†æ¼æ–—æ•°æ® (Funnel)
        # ==========================================
        store_col_f = _pick_col_exact(raw_f, "ä»£ç†å•†") or _pick_any_col(raw_f, ["é—¨åº—", "ç»é”€å•†"]) or raw_f.columns[0]
        name_col_f = _pick_any_col(raw_f, ["ç®¡å®¶", "é¡¾é—®", "é‚€çº¦"]) or raw_f.columns[1]

        col_leads = "çº¿ä¸Š_æœ‰æ•ˆçº¿ç´¢æ•°" if "çº¿ä¸Š_æœ‰æ•ˆçº¿ç´¢æ•°" in raw_f.columns else ("çº¿ç´¢é‡" if "çº¿ç´¢é‡" in raw_f.columns else _pick_any_col(raw_f, ["æœ‰æ•ˆçº¿ç´¢", "çº¿ç´¢æ•°"]))
        col_visits = "çº¿ä¸Š_åˆ°åº—æ•°" if "çº¿ä¸Š_åˆ°åº—æ•°" in raw_f.columns else ("åˆ°åº—é‡" if "åˆ°åº—é‡" in raw_f.columns else _pick_any_col(raw_f, ["åˆ°åº—æ•°", "åˆ°åº—é‡"]))
        col_excel_rate = _pick_any_col(raw_f, ["ç‡"], exclude_keywords=["è¯•é©¾", "æˆäº¤"])

        rename_dict_f = {store_col_f: "é—¨åº—åç§°", name_col_f: "é‚€çº¦ä¸“å‘˜/ç®¡å®¶"}
        if col_leads: rename_dict_f[col_leads] = "çº¿ç´¢é‡"
        if col_visits: rename_dict_f[col_visits] = "åˆ°åº—é‡"
        if col_excel_rate: rename_dict_f[col_excel_rate] = "Excel_Rate"

        df_f = raw_f.rename(columns=rename_dict_f)
        df_f.columns = dedupe_columns(df_f.columns)

        if "é—¨åº—åç§°" in df_f.columns:
            df_f["é—¨åº—åç§°"] = df_f["é—¨åº—åç§°"].replace([r'^\s*$', 'nan', 'None'], np.nan, regex=True).ffill()

        mask_sub = df_f["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"].astype(str).str.contains("å°è®¡|åˆè®¡|æ€»è®¡", na=False)
        df_store_data = df_f[mask_sub].copy()

        mask_bad = df_f["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"].astype(str).str.strip().isin(["", "-", "â€”", "nan", "None"])
        df_advisor_data = df_f[~mask_sub & ~mask_bad].copy()

        for df in [df_store_data, df_advisor_data]:
            if "çº¿ç´¢é‡" in df.columns: df["çº¿ç´¢é‡"] = pd.to_numeric(df["çº¿ç´¢é‡"], errors="coerce").fillna(0)
            else: df["çº¿ç´¢é‡"] = 0.0

            if "åˆ°åº—é‡" in df.columns: df["åˆ°åº—é‡"] = pd.to_numeric(df["åˆ°åº—é‡"], errors="coerce").fillna(0)
            else: df["åˆ°åº—é‡"] = 0.0

            if "Excel_Rate" in df.columns:
                clean_percent_col(df, "Excel_Rate")
                df["çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼"] = df["Excel_Rate"]
            else:
                num = pd.to_numeric(df["åˆ°åº—é‡"], errors="coerce").fillna(0)
                denom = pd.to_numeric(df["çº¿ç´¢é‡"], errors="coerce").fillna(0)
                df["çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼"] = (num / denom).replace([np.inf, -np.inf], 0).fillna(0)

            df["çº¿ç´¢åˆ°åº—ç‡"] = (df["çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼"] * 100).map("{:.1f}%".format)

        store_qc_cols = ["è´¨æ£€æ€»åˆ†", "S_60s", "S_Needs", "S_Car", "S_Policy", "S_Wechat", "S_Time"]
        df_store_data.drop(columns=[c for c in store_qc_cols if c in df_store_data.columns], inplace=True, errors="ignore")

        # ==========================================
        # 2. å¤„ç† DCC é¡¾é—®è´¨æ£€æ•°æ®
        # ==========================================
        df_d = raw_d.rename(columns={
            "é¡¾é—®åç§°": "é‚€çº¦ä¸“å‘˜/ç®¡å®¶", "ç®¡å®¶": "é‚€çº¦ä¸“å‘˜/ç®¡å®¶", "è´¨æ£€æ€»åˆ†": "è´¨æ£€æ€»åˆ†",
            "60ç§’é€šè¯": "S_60s", "ç”¨è½¦éœ€æ±‚": "S_Needs", "è½¦å‹ä¿¡æ¯": "S_Car",
            "æ”¿ç­–ç›¸å…³": "S_Policy", "æ˜ç¡®åˆ°åº—æ—¶é—´": "S_Time"
        })
        store_col_d = _pick_col_exact(raw_d, "é—¨åº—åç§°") or _pick_any_col(raw_d, ["é—¨åº—", "ä»£ç†å•†"])
        if store_col_d and store_col_d in df_d.columns:
             df_d = df_d.rename(columns={store_col_d: "é—¨åº—åç§°"})
        
        df_d.columns = dedupe_columns(df_d.columns)
        
        wechat_cols = [c for c in df_d.columns if ("å¾®ä¿¡" in str(c) and "æ·»åŠ " in str(c)) or ("æ·»åŠ å¾®ä¿¡" in str(c))]
        df_d["S_Wechat"] = _to_1d_numeric(df_d[wechat_cols]) if wechat_cols else 0

        score_cols = ["è´¨æ£€æ€»åˆ†", "S_60s", "S_Needs", "S_Car", "S_Policy", "S_Wechat", "S_Time"]
        for c in score_cols:
            if c in df_d.columns: df_d[c] = pd.to_numeric(df_d[c], errors="coerce")
        
        if "é‚€çº¦ä¸“å‘˜/ç®¡å®¶" not in df_d.columns: df_d["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"] = ""
        cols_to_keep_d = ["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"] + [c for c in score_cols if c in df_d.columns]
        if "é—¨åº—åç§°" in df_d.columns: cols_to_keep_d.append("é—¨åº—åç§°")
        df_d = df_d[cols_to_keep_d]

        # ==========================================
        # 3. å¤„ç† é—¨åº—æ’å/è´¨æ£€æ•°æ®
        # ==========================================
        store_name_candidates = [c for c in raw_s.columns if ("é—¨åº—" in str(c)) and ("ID" not in str(c))]
        store_name_exact = _pick_col_exact(raw_s, "é—¨åº—åç§°")
        
        if store_name_exact: store_name = raw_s[store_name_exact].astype(str)
        elif store_name_candidates:
            tmp = raw_s[store_name_candidates]
            store_name = tmp.astype(str) if isinstance(tmp, pd.Series) else tmp.bfill(axis=1).iloc[:, 0].astype(str)
        else: store_name = pd.Series(["" for _ in range(len(raw_s))])
            
        store_name = store_name.str.strip()
        df_s = pd.DataFrame({"é—¨åº—åç§°": store_name})

        # Mapping config
        col_map = {
            "SR_è´¨æ£€æ€»åˆ†": _pick_any_col(raw_s, ["è´¨æ£€æ€»åˆ†", "æ€»åˆ†"], exclude_keywords=["æ˜¾ç¤º"]),
            "SR_S_60s": _pick_any_col(raw_s, ["60ç§’", "60 ç§’"]),
            "SR_S_Needs": _pick_any_col(raw_s, ["ç”¨è½¦éœ€æ±‚"]),
            "SR_S_Car": _pick_any_col(raw_s, ["è½¦å‹ä¿¡æ¯"]),
            "SR_S_Policy": _pick_any_col(raw_s, ["æ”¿ç­–"]),
            "SR_S_Time": _pick_any_col(raw_s, ["æ˜ç¡®åˆ°åº—", "åˆ°åº—æ—¶é—´"]),
            "SR_S_Wechat": _pick_any_col(raw_s, ["æ·»åŠ å¾®ä¿¡", "åŠ å¾®ä¿¡"])
        }

        for new_col, raw_col in col_map.items():
            if raw_col and raw_col in raw_s.columns:
                df_s[new_col] = _to_1d_numeric(raw_s[raw_col])
            else:
                df_s[new_col] = np.nan

        df_s["é—¨åº—åç§°"] = df_s["é—¨åº—åç§°"].astype(str).str.strip()
        df_s = df_s[df_s["é—¨åº—åç§°"].ne("")].copy()
        df_s = df_s.drop_duplicates(subset=["é—¨åº—åç§°"], keep="first")

        # ==========================================
        # 4. å¤„ç† AMS æ•°æ®
        # ==========================================
        df_a = raw_a.copy()
        store_col_a = _pick_col_exact(raw_a, "ä»£ç†å•†") or _pick_any_col(raw_a, ["é—¨åº—", "ç»é”€å•†"])
        if store_col_a: df_a = df_a.rename(columns={store_col_a: "é—¨åº—åç§°"})

        rename_map_ams = {
            "ç®¡å®¶å§“å": "é‚€çº¦ä¸“å‘˜/ç®¡å®¶", "DCCå¹³å‡é€šè¯æ—¶é•¿": "é€šè¯æ—¶é•¿", "DCCæ¥é€šçº¿ç´¢æ•°": "conn_num",
            "DCCå¤–å‘¼çº¿ç´¢æ•°": "conn_denom", "DCCåŠæ—¶å¤„ç†çº¿ç´¢": "timely_num", "éœ€å¤–å‘¼çº¿ç´¢æ•°": "timely_denom",
            "äºŒæ¬¡å¤–å‘¼çº¿ç´¢æ•°": "call2_num", "éœ€å†å‘¼çº¿ç´¢æ•°": "call2_denom", "DCCä¸‰æ¬¡å¤–å‘¼çš„çº¿ç´¢æ•°": "call3_num",
            "DCCäºŒå‘¼çŠ¶æ€ä¸ºéœ€å†å‘¼çš„çº¿ç´¢æ•°": "call3_denom"
        }
        for src, tgt in rename_map_ams.items():
            if src in df_a.columns: df_a = df_a.rename(columns={src: tgt})

        if "é‚€çº¦ä¸“å‘˜/ç®¡å®¶" not in df_a.columns: df_a["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"] = ""
        
        all_ams_calc_cols = ["conn_num", "conn_denom", "timely_num", "timely_denom",
                             "call2_num", "call2_denom", "call3_num", "call3_denom"]
        for c in all_ams_calc_cols + ["é€šè¯æ—¶é•¿"]:
            if c not in df_a.columns: df_a[c] = 0
            df_a[c] = _to_1d_numeric(df_a[c])

        # ==========================================
        # 5. æ¸…æ´—ä¸åˆå¹¶
        # ==========================================
        for df_x in [df_store_data, df_advisor_data, df_d, df_a, df_s]:
            if "é—¨åº—åç§°" in df_x.columns: df_x["é—¨åº—åç§°"] = strict_clean_str(df_x["é—¨åº—åç§°"])
            if "é‚€çº¦ä¸“å‘˜/ç®¡å®¶" in df_x.columns: df_x["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"] = strict_clean_str(df_x["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"])

        # åˆå¹¶ 1: æ¼æ–—(Advisor) + DCC(Advisor)
        full_advisors = df_advisor_data.copy()
        if "é‚€çº¦ä¸“å‘˜/ç®¡å®¶" in df_d.columns:
            cols_use_d = list(df_d.columns)
            if "é—¨åº—åç§°" in cols_use_d: df_d = df_d.rename(columns={"é—¨åº—åç§°": "é—¨åº—åç§°_dcc"})
            full_advisors = pd.merge(full_advisors, df_d, on="é‚€çº¦ä¸“å‘˜/ç®¡å®¶", how="left", suffixes=("", "_dcc"))

        # åˆå¹¶ 2: åˆå¹¶ AMS (ä¸ªäºº)
        cols_ams_needed = [c for c in all_ams_calc_cols if c in df_a.columns] + ["é€šè¯æ—¶é•¿"]
        join_on = ["é—¨åº—åç§°", "é‚€çº¦ä¸“å‘˜/ç®¡å®¶"] if ("é—¨åº—åç§°" in df_a.columns and "é—¨åº—åç§°" in full_advisors.columns) else ["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"]
        cols_for_merge = list(set(join_on + cols_ams_needed))
        full_advisors = pd.merge(full_advisors, df_a[cols_for_merge], on=join_on, how="left", suffixes=("", "_ams"))

        # å¡«å……0
        for c in ["çº¿ç´¢é‡", "åˆ°åº—é‡", "é€šè¯æ—¶é•¿"] + all_ams_calc_cols:
            if c in full_advisors.columns: full_advisors[c] = pd.to_numeric(full_advisors[c], errors="coerce").fillna(0)

        # ä¸ªäººç‡
        full_advisors["å¤–å‘¼æ¥é€šç‡"] = safe_div(full_advisors, "conn_num", "conn_denom")
        full_advisors["DCCåŠæ—¶å¤„ç†ç‡"] = safe_div(full_advisors, "timely_num", "timely_denom")
        full_advisors["DCCäºŒæ¬¡å¤–å‘¼ç‡"] = safe_div(full_advisors, "call2_num", "call2_denom")
        full_advisors["DCCä¸‰æ¬¡å¤–å‘¼ç‡"] = safe_div(full_advisors, "call3_num", "call3_denom")

        # åˆå¹¶ 3: ç”Ÿæˆ é—¨åº—çº§ å®½è¡¨
        if "é—¨åº—åç§°" in df_a.columns and len(all_ams_calc_cols) > 0:
             ams_store_agg = df_a.groupby("é—¨åº—åç§°").agg({c:"sum" for c in all_ams_calc_cols}).reset_index()
             ams_store_agg["å¤–å‘¼æ¥é€šç‡"] = safe_div(ams_store_agg, "conn_num", "conn_denom")
             ams_store_agg["DCCåŠæ—¶å¤„ç†ç‡"] = safe_div(ams_store_agg, "timely_num", "timely_denom")
             ams_store_agg["DCCäºŒæ¬¡å¤–å‘¼ç‡"] = safe_div(ams_store_agg, "call2_num", "call2_denom")
             ams_store_agg["DCCä¸‰æ¬¡å¤–å‘¼ç‡"] = safe_div(ams_store_agg, "call3_num", "call3_denom")
             
             full_stores = pd.merge(df_store_data, df_s, on="é—¨åº—åç§°", how="left")
             full_stores = pd.merge(full_stores, ams_store_agg, on="é—¨åº—åç§°", how="left")
        else:
             full_stores = pd.merge(df_store_data, df_s, on="é—¨åº—åç§°", how="left")

        # æ¢å¤SR_åˆ—å
        for col in full_stores.columns:
            if str(col).startswith("SR_"):
                real_col = str(col).replace("SR_", "")
                full_stores[real_col] = full_stores[col]
        full_stores.drop(columns=[c for c in full_stores.columns if str(c).startswith("SR_")], inplace=True, errors="ignore")
        full_stores.columns = dedupe_columns(full_stores.columns)

        # ==========================================
        # 6. æ³¨å…¥å½’å±ä¿¡æ¯ (Manager/Province/City)
        # ==========================================
        if df_mapping is not None and not df_mapping.empty:
            # å…³è”é—¨åº—è¡¨
            full_stores["Join_Key"] = strict_clean_str(full_stores["é—¨åº—åç§°"])
            full_stores = pd.merge(full_stores, df_mapping, on="Join_Key", how="left", suffixes=("", "_map"))
            # ä¿®æ­£å…³è”åçš„åˆ—å
            for c in ["åŒºåŸŸç»ç†", "çœä»½", "åŸå¸‚"]:
                if f"{c}_map" in full_stores.columns:
                    full_stores[c] = full_stores[f"{c}_map"].fillna("æœªçŸ¥")
                elif c in full_stores.columns:
                     # å…³é”®ä¿®å¤ï¼šå¦‚æœåŸè¡¨æœ‰åˆ—ä½†æ²¡åŒ¹é…ä¸Šï¼Œä¹Ÿè¦å¡«æœªçŸ¥ï¼Œé˜²æ­¢å‡ºç° NaN
                     full_stores[c] = full_stores[c].fillna("æœªçŸ¥")
                else:
                    full_stores[c] = "æœªçŸ¥"
            
            full_stores.drop(columns=["Join_Key"] + [c for c in full_stores.columns if c.endswith("_map")], inplace=True)
            
            # å…³è”é¡¾é—®è¡¨
            full_advisors["Join_Key"] = strict_clean_str(full_advisors["é—¨åº—åç§°"])
            full_advisors = pd.merge(full_advisors, df_mapping, on="Join_Key", how="left", suffixes=("", "_map"))
            for c in ["åŒºåŸŸç»ç†", "çœä»½", "åŸå¸‚"]:
                if f"{c}_map" in full_advisors.columns:
                    full_advisors[c] = full_advisors[f"{c}_map"].fillna("æœªçŸ¥")
                elif c in full_advisors.columns:
                    full_advisors[c] = full_advisors[c].fillna("æœªçŸ¥")
                else:
                    full_advisors[c] = "æœªçŸ¥"
            
            full_advisors.drop(columns=["Join_Key"] + [c for c in full_advisors.columns if c.endswith("_map")], inplace=True)
        else:
            # å¦‚æœæ²¡æœ‰ Mapping æ–‡ä»¶ï¼Œç»™é»˜è®¤å€¼é˜²æ­¢æŠ¥é”™
            for df in [full_stores, full_advisors]:
                df["åŒºåŸŸç»ç†"] = "æœªçŸ¥"
                df["çœä»½"] = "æœªçŸ¥"
                df["åŸå¸‚"] = "æœªçŸ¥"

        return full_advisors, full_stores

    except Exception as e:
        st.error(f"å¤„ç†å‡ºé”™: {e}")
        st.text(traceback.format_exc())
        return None, None


# --- UI Layout ---

with st.sidebar:
    st.header("âš™ï¸ ç®¡ç†é¢æ¿")

    store_rank_path = get_store_rank_path()
    # æ£€æŸ¥ Operational Data æ˜¯å¦é½å…¨
    op_data_ready = os.path.exists(PATH_F) and os.path.exists(PATH_D) and os.path.exists(PATH_A) and (store_rank_path is not None)
    
    if op_data_ready:
        st.success("âœ… ä¸šåŠ¡æ•°æ®ï¼šå·²å°±ç»ª")
    else:
        st.warning("âš ï¸ ä¸šåŠ¡æ•°æ®ï¼šç¼ºå¤±")
        
    if os.path.exists(PATH_M):
        st.success("âœ… å½’å±æ•°æ®ï¼šå·²å°±ç»ª")
    else:
        st.warning("âš ï¸ å½’å±æ•°æ®ï¼šæš‚æ—  (è¯·ä¸Šä¼ )")
        
    st.markdown("---")

    with st.expander("ğŸ” æ›´æ–°æ•°æ® (ä»…é™ç®¡ç†å‘˜)"):
        pwd = st.text_input("è¾“å…¥ç®¡ç†å‘˜å¯†ç ", type="password")
        if pwd == ADMIN_PASSWORD:
            tab1, tab2 = st.tabs(["ğŸ“Š æ›´æ–°ä¸šåŠ¡æ•°æ®", "ğŸ—ºï¸ æ›´æ–°å½’å±å…³ç³»"])
            
            with tab1:
                st.info("è¯·ä¸Šä¼ æœ¬æ¬¡è€ƒè¯„å‘¨æœŸçš„ 4 ä¸ªä¸šåŠ¡æŠ¥è¡¨ï¼š")
                new_f = st.file_uploader("1. æ¼æ–—æŒ‡æ ‡è¡¨", type=["xlsx", "csv"], key="up_f")
                new_d = st.file_uploader("2. é¡¾é—®è´¨æ£€è¡¨", type=["xlsx", "csv"], key="up_d")
                new_a = st.file_uploader("3. AMSè·Ÿè¿›è¡¨", type=["xlsx", "csv"], key="up_a")
                new_s = st.file_uploader("4. é—¨åº—æ’åè¡¨", type=["xlsx", "csv"], key="up_s")

                if st.button("ğŸš€ æäº¤ä¸šåŠ¡æ•°æ®"):
                    if new_f and new_d and new_a and new_s:
                        with st.spinner("æ­£åœ¨ä¿å­˜ä¸šåŠ¡æ•°æ®..."):
                            save_uploaded_file(new_f, PATH_F)
                            save_uploaded_file(new_d, PATH_D)
                            save_uploaded_file(new_a, PATH_A)
                            
                            # é—¨åº—æ’åç‰¹æ®Šå¤„ç†
                            if str(new_s.name).lower().endswith(".xlsx"):
                                if os.path.exists(PATH_S_CSV): os.remove(PATH_S_CSV)
                                save_uploaded_file(new_s, PATH_S_XLSX)
                            else:
                                if os.path.exists(PATH_S_XLSX): os.remove(PATH_S_XLSX)
                                save_uploaded_file(new_s, PATH_S_CSV)

                            try:
                                with open(LAST_UPDATE_FILE, "w", encoding="utf-8") as f:
                                    f.write(datetime.now().isoformat(timespec="seconds"))
                            except Exception: pass
                        
                        # æ¸…é™¤ç¼“å­˜ï¼Œç¡®ä¿ä¸‹æ¬¡è¯»å–æ–°æ•°æ®
                        process_data.clear()
                        st.success("æ›´æ–°å®Œæˆï¼Œæ­£åœ¨åˆ·æ–°...")
                        st.rerun()
                    else:
                        st.error("è¯·ä¼ é½ 4 ä¸ªä¸šåŠ¡æ–‡ä»¶")
            
            with tab2:
                st.info("æ­¤å¤„ä¸Šä¼ ã€ä»£ç†å•†åç§°å½’å±è¡¨ã€‘ã€‚ä»…éœ€ä¸Šä¼ ä¸€æ¬¡ï¼Œé™¤éå½’å±å…³ç³»å‘ç”Ÿå˜æ›´ã€‚")
                new_m = st.file_uploader("5. ä»£ç†å•†å½’å±è¡¨ (å«åŒºåŸŸ/çœä»½/åŸå¸‚)", type=["xlsx", "csv"], key="up_m")
                
                if st.button("ğŸ’¾ ä¿å­˜å½’å±å…³ç³»"):
                    if new_m:
                        with st.spinner("æ­£åœ¨ä¿å­˜å½’å±è¡¨..."):
                            save_uploaded_file(new_m, PATH_M)
                        # æ¸…é™¤ç¼“å­˜ï¼Œç¡®ä¿ä¸‹æ¬¡è¯»å–æ–°å½’å±å…³ç³»
                        process_data.clear()
                        st.success("å½’å±å…³ç³»å·²æ›´æ–°ï¼")
                        st.rerun()
                    else:
                        st.error("è¯·é€‰æ‹©æ–‡ä»¶")


store_rank_path = get_store_rank_path()
op_data_ready = os.path.exists(PATH_F) and os.path.exists(PATH_D) and os.path.exists(PATH_A) and (store_rank_path is not None)

if op_data_ready:
    # æ— è®ºæœ‰æ—  Mapping æ–‡ä»¶ï¼Œéƒ½å°è¯•è¯»å– (process_data å†…éƒ¨å¤„ç†äº† None çš„æƒ…å†µ)
    df_advisors, df_stores = process_data(PATH_F, PATH_D, PATH_A, store_rank_path, PATH_M)

    if df_advisors is not None:
        col_header, col_update = st.columns([3, 1])
        with col_header:
            st.title("Audi | DCC æ•ˆèƒ½çœ‹æ¿")
        with col_update:
            upd = get_data_update_time(store_rank_path)
            upd_text = upd.strftime("%Y-%m-%d %H:%M") if upd else "æš‚æ— "
            st.markdown(f"<div style='text-align:right;color:gray;font-size:12px;padding-top:20px;'>ğŸ•’ æ•°æ®æ›´æ–°: {upd_text}</div>", unsafe_allow_html=True)

        # =========================================================
        # å››çº§è”åŠ¨ç­›é€‰å™¨ (Cascading Filters)
        # =========================================================
        st.markdown("### ğŸ§¬ å¤šç»´è§†å›¾åˆ‡æ¢")
        
        # ä½¿ç”¨ 4 åˆ—å¸ƒå±€ï¼Œä½¿ç­›é€‰æ¡†ç­‰å®½ã€æ•´é½
        f_c1, f_c2, f_c3, f_c4 = st.columns(4)
        
        # 1. åŒºåŸŸç»ç†
        # å¼ºåˆ¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²å†æå– uniqueï¼Œé˜²æ­¢ TypeError (NaN vs String)
        if "åŒºåŸŸç»ç†" in df_stores.columns:
            mgr_list = sorted(df_stores["åŒºåŸŸç»ç†"].dropna().astype(str).unique().tolist())
        else:
            mgr_list = []
        all_managers = ["å…¨éƒ¨"] + mgr_list

        with f_c1:
            sel_mgr = st.selectbox("1ï¸âƒ£ åŒºåŸŸç»ç†", all_managers, key="filter_mgr")
        
        # 2. çœä»½ (åŸºäºç»ç†è”åŠ¨)
        df_l2 = df_stores if sel_mgr == "å…¨éƒ¨" else df_stores[df_stores["åŒºåŸŸç»ç†"] == sel_mgr]
        if "çœä»½" in df_l2.columns:
            prov_list = sorted(df_l2["çœä»½"].dropna().astype(str).unique().tolist())
        else:
            prov_list = []
        all_provs = ["å…¨éƒ¨"] + prov_list
        
        with f_c2:
            sel_prov = st.selectbox("2ï¸âƒ£ çœä»½", all_provs, key="filter_prov")
        
        # 3. åŸå¸‚ (åŸºäºçœä»½è”åŠ¨)
        df_l3 = df_l2 if sel_prov == "å…¨éƒ¨" else df_l2[df_l2["çœä»½"] == sel_prov]
        if "åŸå¸‚" in df_l3.columns:
            city_list = sorted(df_l3["åŸå¸‚"].dropna().astype(str).unique().tolist())
        else:
            city_list = []
        all_cities = ["å…¨éƒ¨"] + city_list
        
        with f_c3:
            sel_city = st.selectbox("3ï¸âƒ£ åŸå¸‚", all_cities, key="filter_city")
        
        # 4. é—¨åº— (åŸºäºåŸå¸‚è”åŠ¨)
        df_l4 = df_l3 if sel_city == "å…¨éƒ¨" else df_l3[df_l3["åŸå¸‚"] == sel_city]
        if "é—¨åº—åç§°" in df_l4.columns:
            store_list = sorted(df_l4["é—¨åº—åç§°"].dropna().astype(str).unique().tolist())
        else:
            store_list = []
        all_stores = ["å…¨éƒ¨"] + store_list

        with f_c4:
            sel_store = st.selectbox("4ï¸âƒ£ é—¨åº—", all_stores, key="filter_store")

        # =========================================================
        # æ•°æ®è¿‡æ»¤é€»è¾‘
        # =========================================================
        
        filtered_stores = df_l4.copy() # å½“å‰ç­›é€‰æ¼æ–—å‰©ä¸‹çš„æ‰€æœ‰é—¨åº—
        
        if sel_store == "å…¨éƒ¨":
            # èšåˆæ¨¡å¼ï¼šè®¡ç®— filtered_stores çš„æ€»å’Œ/å¹³å‡
            current_df = filtered_stores.copy()
            
            # æ˜¾ç¤ºæ ‡é¢˜
            if sel_city != "å…¨éƒ¨": rank_title = f"ğŸ† {sel_city} - é—¨åº—æ’å"
            elif sel_prov != "å…¨éƒ¨": rank_title = f"ğŸ† {sel_prov} - é—¨åº—æ’å"
            elif sel_mgr != "å…¨éƒ¨": rank_title = f"ğŸ† {sel_mgr}åŒºåŸŸ - é—¨åº—æ’å"
            else: rank_title = "ğŸ† å…¨åŒºé—¨åº—æ’å"
            
            # ç”¨äº KPI è®¡ç®—çš„èšåˆ
            kpi_leads = current_df["çº¿ç´¢é‡"].sum()
            kpi_visits = current_df["åˆ°åº—é‡"].sum()
            kpi_rate = kpi_visits / kpi_leads if kpi_leads > 0 else 0
            kpi_score = current_df["è´¨æ£€æ€»åˆ†"].mean() # å¹³å‡åˆ†
            
            # ç”¨äºç»˜å›¾çš„ DF å°±æ˜¯ current_df (åŒ…å«è¯¥å±‚çº§ä¸‹æ‰€æœ‰é—¨åº—)
            current_df["åç§°"] = current_df["é—¨åº—åç§°"]
            
        else:
            # å•åº—æ¨¡å¼
            current_df = df_advisors[df_advisors["é—¨åº—åç§°"] == sel_store].copy()
            current_df["åç§°"] = current_df["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"]
            rank_title = f"ğŸ‘¤ {sel_store} - é¡¾é—®æ’å"
            
            kpi_leads = current_df["çº¿ç´¢é‡"].sum()
            kpi_visits = current_df["åˆ°åº—é‡"].sum()
            kpi_rate = kpi_visits / kpi_leads if kpi_leads > 0 else 0
            kpi_score = current_df["è´¨æ£€æ€»åˆ†"].mean()

        # =========================================================
        # ä»ªè¡¨ç›˜å±•ç¤º
        # =========================================================
        st.subheader("1ï¸âƒ£ ç»“æœæ¦‚è§ˆ (Result)")
        k1, k2, k3, k4 = st.columns(4)
        k1.metric("æ€»æœ‰æ•ˆçº¿ç´¢", f"{int(kpi_leads):,}")
        k2.metric("æ€»å®é™…åˆ°åº—", f"{int(kpi_visits):,}")
        k3.metric("çº¿ç´¢åˆ°åº—ç‡", f"{kpi_rate:.1%}")
        k4.metric("å¹³å‡è´¨æ£€æ€»åˆ†", f"{kpi_score:.1f}")

        st.markdown("---")
        st.subheader("2ï¸âƒ£ DCC å¤–å‘¼è¿‡ç¨‹ç›‘æ§ (Process)")

        def calc_kpi_rate(df, num, denom):
            if num not in df.columns or denom not in df.columns: return 0
            total_num = df[num].sum()
            total_denom = df[denom].sum()
            return total_num / total_denom if total_denom > 0 else 0

        p1, p2, p3, p4 = st.columns(4)
        avg_conn = calc_kpi_rate(current_df, "conn_num", "conn_denom")
        avg_timely = calc_kpi_rate(current_df, "timely_num", "timely_denom")
        avg_call2 = calc_kpi_rate(current_df, "call2_num", "call2_denom")
        avg_call3 = calc_kpi_rate(current_df, "call3_num", "call3_denom")

        p1.metric("ğŸ“ å¤–å‘¼æ¥é€šç‡", f"{avg_conn:.1%}")
        p2.metric("âš¡ DCCåŠæ—¶å¤„ç†ç‡", f"{avg_timely:.1%}")
        p3.metric("ğŸ”„ äºŒæ¬¡å¤–å‘¼ç‡", f"{avg_call2:.1%}")
        p4.metric("ğŸ” ä¸‰æ¬¡å¤–å‘¼ç‡", f"{avg_call3:.1%}")
        
        # ç»˜å›¾æ•°æ®å‡†å¤‡
        plot_df_vis = current_df.copy()
        plot_df_vis["è´¨æ£€æ€»åˆ†_æ˜¾ç¤º"] = plot_df_vis.get("è´¨æ£€æ€»åˆ†", pd.Series([0]*len(plot_df_vis))).fillna(0)

        # æ•£ç‚¹å›¾ 1
        c_proc_1, c_proc_2 = st.columns(2)
        with c_proc_1:
            st.markdown("#### ğŸ•µï¸ å¼‚å¸¸ä¾¦æµ‹ï¼šå¤–å‘¼æ¥é€šç‡ vs 60ç§’é€šè¯å æ¯”")
            if "S_60s" in plot_df_vis.columns and "å¤–å‘¼æ¥é€šç‡" in plot_df_vis.columns:
                fig_p1 = px.scatter(
                    plot_df_vis, x="å¤–å‘¼æ¥é€šç‡", y="S_60s",
                    size="çº¿ç´¢é‡", color="è´¨æ£€æ€»åˆ†_æ˜¾ç¤º", hover_name="åç§°",
                    color_continuous_scale="RdYlGn", height=350,
                )
                fig_p1.add_vline(x=avg_conn, line_dash="dash", line_color="gray")
                fig_p1.update_layout(xaxis=dict(tickformat=".0%"))
                st.plotly_chart(fig_p1, use_container_width=True)
            else: st.warning("æ•°æ®ä¸è¶³")

        with c_proc_2:
            st.markdown("#### ğŸ”— å½’å› åˆ†æï¼šè¿‡ç¨‹æŒ‡æ ‡ vs çº¿ç´¢é¦–é‚€åˆ°åº—ç‡")
            x_axis_choice = st.radio("é€‰æ‹©æ¨ªè½´æŒ‡æ ‡ï¼š", ["DCCåŠæ—¶å¤„ç†ç‡", "DCCäºŒæ¬¡å¤–å‘¼ç‡", "DCCä¸‰æ¬¡å¤–å‘¼ç‡"], horizontal=True)
            
            plot_df_vis["çº¿ç´¢åˆ°åº—ç‡_æ˜¾ç¤º"] = pd.to_numeric(plot_df_vis.get("çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼", 0)).fillna(0).clip(0, 1)
            
            if x_axis_choice in plot_df_vis.columns:
                fig_p2 = px.scatter(
                    plot_df_vis, x=x_axis_choice, y="çº¿ç´¢åˆ°åº—ç‡_æ˜¾ç¤º",
                    size="çº¿ç´¢é‡", color="è´¨æ£€æ€»åˆ†_æ˜¾ç¤º", hover_name="åç§°",
                    color_continuous_scale="Blues", height=300
                )
                fig_p2.update_layout(xaxis=dict(tickformat=".0%"), yaxis=dict(tickformat=".1%"))
                st.plotly_chart(fig_p2, use_container_width=True)
            else: st.warning("æ•°æ®ä¸è¶³")

        st.markdown("---")

        c_left, c_right = st.columns([1, 2])
        with c_left:
            st.markdown(f"### {rank_title}")
            if "çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼" in current_df.columns:
                rank_df = current_df[["åç§°", "çº¿ç´¢åˆ°åº—ç‡", "çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼", "è´¨æ£€æ€»åˆ†"]].copy()
                rank_df["Sort_Score"] = rank_df["çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼"].fillna(-1)
                rank_df = rank_df.sort_values("Sort_Score", ascending=False).head(15)
                st.dataframe(
                    rank_df[["åç§°", "çº¿ç´¢åˆ°åº—ç‡", "è´¨æ£€æ€»åˆ†"]],
                    hide_index=True, use_container_width=True, height=400,
                    column_config={"è´¨æ£€æ€»åˆ†": st.column_config.NumberColumn(format="%.1f")}
                )
            else: st.warning("æ— æ’è¡Œæ•°æ®")

        with c_right:
            st.markdown("### ğŸ’¡ è¯æœ¯è´¨é‡ vs è½¬åŒ–ç»“æœ")
            if "S_Time" in plot_df_vis.columns:
                fig = px.scatter(
                    plot_df_vis, x="S_Time", y="çº¿ç´¢åˆ°åº—ç‡_æ˜¾ç¤º",
                    size="çº¿ç´¢é‡", color="è´¨æ£€æ€»åˆ†_æ˜¾ç¤º", hover_name="åç§°",
                    color_continuous_scale="Reds", height=400,
                    labels={"S_Time": "æ˜ç¡®åˆ°åº—æ—¶é—´å¾—åˆ†", "çº¿ç´¢åˆ°åº—ç‡_æ˜¾ç¤º": "çº¿ç´¢åˆ°åº—ç‡"}
                )
                fig.update_layout(yaxis=dict(tickformat=".1%"))
                st.plotly_chart(fig, use_container_width=True)
            else: st.warning("æ•°æ®ä¸è¶³")

        # è¯Šæ–­éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œä»…å½“é€‰äº†å…·ä½“é—¨åº—æ—¶æ‰æ˜¾ç¤ºé¡¾é—®åˆ—è¡¨
        st.markdown("---")
        if sel_store != "å…¨éƒ¨":
            st.markdown("### ğŸ•µï¸â€â™€ï¸ é¡¾é—®æ·±åº¦è¯Šæ–­")
            # ... (è¿™éƒ¨åˆ†é€»è¾‘å¤ç”¨ä¹‹å‰çš„ï¼Œæ— éœ€å˜åŠ¨ï¼Œå› ä¸ºcurrent_dfå·²ç»æ˜¯é¡¾é—®çº§æ•°æ®)
            diag_list = sorted(current_df["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"].dropna().astype(str).unique())
            if diag_list:
                sel_p = st.selectbox("ğŸ” é€‰æ‹©é¡¾é—®ï¼š", diag_list)
                p_row = current_df[current_df["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"] == sel_p]
                if not p_row.empty:
                    p = p_row.iloc[0]
                    # ç®€å•ç»˜åˆ¶ä¸€ä¸‹æ¼æ–—
                    c1, c2 = st.columns(2)
                    with c1:
                        fig_f = go.Figure(go.Funnel(y=["çº¿ç´¢", "åˆ°åº—"], x=[p.get("çº¿ç´¢é‡",0), p.get("åˆ°åº—é‡",0)]))
                        fig_f.update_layout(height=200, showlegend=False)
                        st.plotly_chart(fig_f, use_container_width=True)
                    with c2:
                        st.metric("çº¿ç´¢åˆ°åº—ç‡", p.get("çº¿ç´¢åˆ°åº—ç‡", "0%"))
                        st.metric("è´¨æ£€æ€»åˆ†", f"{p.get('è´¨æ£€æ€»åˆ†', 0):.1f}")
        else:
            st.info("ğŸ’¡ é€‰æ‹©å…·ä½“ã€é—¨åº—ã€‘åï¼Œå¯æŸ¥çœ‹è¯¥åº—é¡¾é—®çš„è¯¦ç»†è¯Šæ–­æŠ¥å‘Šã€‚")

else:
    st.info("ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ Audi æ•ˆèƒ½çœ‹æ¿ï¼")
    st.warning("ğŸ‘‰ è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ æ•°æ®ã€‚")
