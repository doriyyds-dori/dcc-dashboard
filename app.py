import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
import os
from datetime import datetime

# ================= 1. é¡µé¢é…ç½® =================
st.set_page_config(page_title="Audi DCC æ•ˆèƒ½çœ‹æ¿", layout="wide", page_icon="ğŸï¸")

st.markdown(
    """
<style>
    .top-container {display: flex; align-items: center; justify-content: space-between; padding-bottom: 20px; border-bottom: 2px solid #f0f0f0;}
    .metric-card {background-color: #fff; border: 1px solid #e0e0e0; border-radius: 8px; padding: 15px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);}
    div[data-testid="stSelectbox"] {min-width: 200px;}
    .big-font {font-size: 18px !important; font-weight: bold;}
</style>
""",
    unsafe_allow_html=True,
)

# ================= 2. å®‰å…¨é”ã€æ–‡ä»¶å­˜å‚¨ä¸é‚®ä»¶é…ç½® =================
ADMIN_PASSWORD = "AudiSARR3"

DATA_DIR = "data_store"
os.makedirs(DATA_DIR, exist_ok=True)

# 1) æ¼æ–— / 2) é¡¾é—®è´¨æ£€ / 3) AMS
PATH_F = os.path.join(DATA_DIR, "funnel.xlsx")
PATH_D = os.path.join(DATA_DIR, "dcc.xlsx")
PATH_A = os.path.join(DATA_DIR, "ams.xlsx")

# âœ… 4) é—¨åº—æ’åï¼šçœŸå®åç¼€ä¿å­˜ï¼Œè¯»å–æ—¶è‡ªåŠ¨é€‰å­˜åœ¨çš„é‚£ä¸ª
PATH_S_XLSX = os.path.join(DATA_DIR, "store_rank.xlsx")
PATH_S_CSV = os.path.join(DATA_DIR, "store_rank.csv")

# âœ… 5) é—¨åº—å½’å±ç»´è¡¨ï¼šç½‘é¡µç«¯ä¸Šä¼ åä¿å­˜åˆ° data_storeï¼ˆä¸€æ¬¡ä¸Šä¼ ï¼Œåç»­è‡ªåŠ¨è¯»å–ï¼‰
PATH_M = os.path.join(DATA_DIR, "store_map.xlsx")
# å…¼å®¹å…œåº•ï¼ˆå¯é€‰ï¼‰ï¼šæœåŠ¡å™¨æœ¬åœ°å›ºå®šæ”¾ç½®çš„å½’å±è¡¨è·¯å¾„
STORE_MAP_FALLBACK = "/mnt/data/ä»£ç†å•†åç§°å½’å±.xlsx"


def _resolve_store_map_path():
    if os.path.exists(PATH_M):
        return PATH_M
    if os.path.exists(STORE_MAP_FALLBACK):
        return STORE_MAP_FALLBACK
    return None


def get_store_map_df():
    """è¯»å–é—¨åº—å½’å±è¡¨ï¼›è‹¥ä¸å­˜åœ¨æˆ–åˆ—ä¸é½ï¼Œè¿”å› Noneï¼ˆè‡ªåŠ¨å›é€€åˆ°æ—§çš„é—¨åº—ä¸‹æ‹‰ï¼‰ã€‚"""
    map_path = _resolve_store_map_path()
    if not map_path:
        return None

    try:
        m = pd.read_excel(map_path)
        m.columns = m.columns.astype(str).str.strip()

        # å…¼å®¹ï¼šæœ‰äº›æ–‡ä»¶ç”¨â€œå•†åŠ¡ç»ç†â€è€Œä¸æ˜¯â€œåŒºåŸŸç»ç†â€
        if "å•†åŠ¡ç»ç†" in m.columns and "åŒºåŸŸç»ç†" not in m.columns:
            m = m.rename(columns={"å•†åŠ¡ç»ç†": "åŒºåŸŸç»ç†"})

        need_cols = {"åŒºåŸŸç»ç†", "çœä»½", "åŸå¸‚", "é—¨åº—åç§°"}
        if not need_cols.issubset(set(m.columns)):
            return None

        for c in ["åŒºåŸŸç»ç†", "çœä»½", "åŸå¸‚", "é—¨åº—åç§°"]:
            m[c] = m[c].astype(str).str.strip()

        m = m[m["é—¨åº—åç§°"].notna() & (m["é—¨åº—åç§°"].astype(str).str.strip() != "")]
        m = m.drop_duplicates(subset=["åŒºåŸŸç»ç†", "çœä»½", "åŸå¸‚", "é—¨åº—åç§°"])
        return m
    except Exception:
        return None


def save_uploaded_file(uploaded_file, save_path: str) -> bool:
    try:
        with open(save_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        return True
    except Exception as e:
        st.error(f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
        return False


def get_store_rank_path():
    if os.path.exists(PATH_S_XLSX):
        return PATH_S_XLSX
    if os.path.exists(PATH_S_CSV):
        return PATH_S_CSV
    return None


LAST_UPDATE_FILE = os.path.join(DATA_DIR, "_last_upload_time.txt")


def get_data_update_time(store_rank_path: str | None):
    """è¿”å›ã€æœ€æ–°ä¸€æ¬¡ä¸Šä¼ æ•°æ®æŠ¥ã€‘çš„æ—¶é—´ã€‚ä¼˜å…ˆè¯» _last_upload_time.txtï¼Œå¦åˆ™å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´ã€‚"""
    if os.path.exists(LAST_UPDATE_FILE):
        try:
            txt = open(LAST_UPDATE_FILE, "r", encoding="utf-8").read().strip()
            if txt:
                return datetime.fromisoformat(txt)
        except Exception:
            pass

    paths = [PATH_F, PATH_D, PATH_A]
    if store_rank_path:
        paths.append(store_rank_path)

    mtimes = []
    for p in paths:
        if p and os.path.exists(p):
            try:
                mtimes.append(os.path.getmtime(p))
            except Exception:
                pass

    if not mtimes:
        return None

    ts = max(mtimes)
    return datetime.fromtimestamp(ts)


# ================= 3. å·¥å…·å‡½æ•°ï¼ˆè¯»å–/æ¸…æ´—/è®¡ç®—ï¼‰ =================
def dedupe_columns(columns):
    """æŠŠé‡å¤åˆ—åå˜æˆ: åˆ—å, åˆ—å__1, åˆ—å__2 ..."""
    seen = {}
    out = []
    for c in list(columns):
        c = str(c)
        if c not in seen:
            seen[c] = 0
            out.append(c)
        else:
            seen[c] += 1
            out.append(f"{c}__{seen[c]}")
    return out


def smart_read(file_path: str, is_rank_file: bool = False, prefer_sheet: str | None = None, require_sheet: bool = False):
    """é²æ£’è¯»å–ï¼ˆxlsx/csv/è¯¯åç¼€ xlsxï¼‰+ è‡ªåŠ¨æ‰¾è¡¨å¤´ + åˆ—åå»é‡ã€‚

    ä¸¥æ ¼æ¨¡å¼ï¼š
    - è‹¥æ˜¯ Excel ä¸” require_sheet=True ä¸” prefer_sheet ä¸å­˜åœ¨ï¼šç›´æ¥æŠ›é”™ï¼Œä¸å›é€€ã€‚
    """
    if not file_path or not os.path.exists(file_path):
        return None

    df = None

    def _read_excel_with_prefer(path: str):
        xls = pd.ExcelFile(path)
        if prefer_sheet:
            if prefer_sheet in xls.sheet_names:
                return pd.read_excel(xls, sheet_name=prefer_sheet, header=None)
            if require_sheet:
                raise ValueError(f"æ¼æ–—æŒ‡æ ‡æ–‡ä»¶ç¼ºå°‘æŒ‡å®šSheetï¼š'{prefer_sheet}'ã€‚å½“å‰åŒ…å«ï¼š{xls.sheet_names}")
            # éä¸¥æ ¼ï¼šå›é€€ç¬¬ä¸€ä¸ªsheetï¼ˆæœ¬æ¬¡ä½ ä¸éœ€è¦ï¼Œä½†ä¿ç•™èƒ½åŠ›ï¼‰
            return pd.read_excel(xls, sheet_name=0, header=None)
        return pd.read_excel(xls, sheet_name=0, header=None)

    # å…œåº•ï¼šç­¾ååˆ¤æ–­ï¼ˆxlsx æ˜¯ zipï¼šPK..ï¼‰
    try:
        with open(file_path, "rb") as f:
            sig = f.read(4)
        if sig == b"PK\x03\x04":
            df = _read_excel_with_prefer(file_path)
    except Exception as e:
        # ä¸¥æ ¼æ¨¡å¼ä¸‹ï¼Œç¼ºsheetè¦æŠŠé”™è¯¯æŠ›å‡ºå»ç»™ä¸Šå±‚æ˜¾ç¤º
        if require_sheet and prefer_sheet:
            raise
        # éä¸¥æ ¼ï¼šç»§ç»­èµ°åé¢åˆ†æ”¯
        df = None

    if df is None:
        is_csv = str(file_path).lower().endswith((".csv", ".txt"))
        if is_csv:
            encodings = ["utf-8-sig", "gb18030", "utf-16"]
            for enc in encodings:
                try:
                    df = pd.read_csv(file_path, header=None, encoding=enc, engine="python", on_bad_lines="skip")
                    break
                except (UnicodeDecodeError, pd.errors.ParserError):
                    continue
                except Exception:
                    continue
        else:
            try:
                df = _read_excel_with_prefer(file_path)
            except Exception:
                if require_sheet and prefer_sheet:
                    raise
                return None

    if df is None or df.empty:
        return None

    # æ™ºèƒ½æ‰¾è¡¨å¤´
    keywords = ["é—¨åº—", "é¡¾é—®", "ç®¡å®¶", "æ’å", "ä»£ç†å•†", "åºå·", "çº¿ç´¢", "è´¨æ£€", "æ·»åŠ å¾®ä¿¡"]
    header_row = 0

    search_rows = 15 if is_rank_file else 12
    for i in range(min(search_rows, len(df))):
        row_values = df.iloc[i].astype(str).str.cat(sep=",")
        if any(k in row_values for k in keywords):
            header_row = i
            break

    df.columns = df.iloc[header_row]
    df = df[header_row + 1 :].reset_index(drop=True)

    df.columns = (
        df.columns.astype(str)
        .str.strip()
        .str.replace("\n", "", regex=False)
        .str.replace("\r", "", regex=False)
    )

    df.columns = dedupe_columns(df.columns)

    df = df.loc[:, df.columns.notna()]
    df = df.loc[:, df.columns != "nan"]

    return df


def clean_percent_col(df: pd.DataFrame, col_name: str):
    if col_name not in df.columns:
        return
    series = df[col_name].astype(str).str.strip().str.replace("%", "", regex=False)
    numeric_series = pd.to_numeric(series, errors="coerce").fillna(0)
    if numeric_series.max() > 1.0:
        df[col_name] = numeric_series / 100
    else:
        df[col_name] = numeric_series


def safe_div(df: pd.DataFrame, num_col: str, denom_col: str):
    if num_col not in df.columns or denom_col not in df.columns:
        return 0
    num = pd.to_numeric(df[num_col], errors="coerce").fillna(0)
    denom = pd.to_numeric(df[denom_col], errors="coerce").fillna(0)
    return (num / denom).replace([np.inf, -np.inf], 0).fillna(0)


def _to_1d_numeric(x):
    if isinstance(x, pd.DataFrame):
        tmp = x.apply(pd.to_numeric, errors="coerce")
        return tmp.bfill(axis=1).iloc[:, 0].fillna(0)
    return pd.to_numeric(x, errors="coerce").fillna(0)


def _pick_any_col(df: pd.DataFrame, any_keywords, exclude_keywords=None):
    exclude_keywords = exclude_keywords or []
    for c in df.columns:
        s = str(c)
        if any(k in s for k in any_keywords) and not any(x in s for x in exclude_keywords):
            return c
    return None


def _col_as_series(df: pd.DataFrame, col_name: str):
    if col_name not in df.columns:
        return None
    x = df[col_name]
    if isinstance(x, pd.DataFrame):
        x = x.iloc[:, 0]
    return x


@st.cache_data(ttl=300)
def process_data(path_f, path_d, path_a, path_s):
    try:
        # âœ… ä¸¥æ ¼ï¼šæ¼æ–—åªè¯» sheet="æ¼æ–—æŒ‡æ ‡"ï¼Œæ‰¾ä¸åˆ°ç›´æ¥æŠ¥é”™
        raw_f = smart_read(path_f, prefer_sheet="æ¼æ–—æŒ‡æ ‡", require_sheet=True)
        raw_d = smart_read(path_d)
        raw_a = smart_read(path_a)
        raw_s = smart_read(path_s, is_rank_file=True)

        if raw_f is None or raw_d is None or raw_a is None or raw_s is None:
            return None, None

        # ================= A. Funnel (æ¼æ–—) =================
        store_col = _pick_any_col(raw_f, ["ä»£ç†å•†", "é—¨åº—"]) or raw_f.columns[0]
        name_col = _pick_any_col(raw_f, ["ç®¡å®¶", "é¡¾é—®", "é‚€çº¦"]) or raw_f.columns[1]

        col_leads = "çº¿ä¸Š_æœ‰æ•ˆçº¿ç´¢æ•°" if "çº¿ä¸Š_æœ‰æ•ˆçº¿ç´¢æ•°" in raw_f.columns else ("çº¿ç´¢é‡" if "çº¿ç´¢é‡" in raw_f.columns else _pick_any_col(raw_f, ["æœ‰æ•ˆçº¿ç´¢", "çº¿ç´¢æ•°"]))
        col_visits = "çº¿ä¸Š_åˆ°åº—æ•°" if "çº¿ä¸Š_åˆ°åº—æ•°" in raw_f.columns else ("åˆ°åº—é‡" if "åˆ°åº—é‡" in raw_f.columns else _pick_any_col(raw_f, ["åˆ°åº—æ•°", "åˆ°åº—é‡"]))
        col_excel_rate = _pick_any_col(raw_f, ["ç‡"], exclude_keywords=["è¯•é©¾", "æˆäº¤"])

        rename_dict = {store_col: "é—¨åº—åç§°", name_col: "é‚€çº¦ä¸“å‘˜/ç®¡å®¶"}
        if col_leads:
            rename_dict[col_leads] = "çº¿ç´¢é‡"
        if col_visits:
            rename_dict[col_visits] = "åˆ°åº—é‡"
        if col_excel_rate:
            rename_dict[col_excel_rate] = "Excel_Rate"

        df_f = raw_f.rename(columns=rename_dict)
        df_f.columns = dedupe_columns(df_f.columns)

        mask_sub = df_f["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"].astype(str).str.contains("å°è®¡|åˆè®¡|æ€»è®¡", na=False)
        df_store_data = df_f[mask_sub].copy()

        mask_bad = df_f["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"].astype(str).str.strip().isin(["", "-", "â€”", "nan"])
        df_advisor_data = df_f[~mask_sub & ~mask_bad].copy()

        for df in [df_store_data, df_advisor_data]:
            df["çº¿ç´¢é‡"] = pd.to_numeric(df.get("çº¿ç´¢é‡", 0), errors="coerce").fillna(0)
            df["åˆ°åº—é‡"] = pd.to_numeric(df.get("åˆ°åº—é‡", 0), errors="coerce").fillna(0)

            if "Excel_Rate" in df.columns:
                clean_percent_col(df, "Excel_Rate")
                df["çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼"] = df["Excel_Rate"]
            else:
                df["çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼"] = safe_div(df, "åˆ°åº—é‡", "çº¿ç´¢é‡")

            df["çº¿ç´¢åˆ°åº—ç‡"] = (df["çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼"] * 100).map("{:.1f}%".format)

        store_qc_cols = ["è´¨æ£€æ€»åˆ†", "S_60s", "S_Needs", "S_Car", "S_Policy", "S_Wechat", "S_Time"]
        df_store_data.drop(columns=[c for c in store_qc_cols if c in df_store_data.columns], inplace=True, errors="ignore")

        # ================= B. DCC (é¡¾é—®è´¨æ£€) =================
        df_d = raw_d.rename(
            columns={
                "é¡¾é—®åç§°": "é‚€çº¦ä¸“å‘˜/ç®¡å®¶",
                "ç®¡å®¶": "é‚€çº¦ä¸“å‘˜/ç®¡å®¶",
                "è´¨æ£€æ€»åˆ†": "è´¨æ£€æ€»åˆ†",
                "60ç§’é€šè¯": "S_60s",
                "ç”¨è½¦éœ€æ±‚": "S_Needs",
                "è½¦å‹ä¿¡æ¯": "S_Car",
                "æ”¿ç­–ç›¸å…³": "S_Policy",
                "æ˜ç¡®åˆ°åº—æ—¶é—´": "S_Time",
            }
        )
        df_d.columns = dedupe_columns(df_d.columns)

        wechat_cols = [c for c in df_d.columns if ("å¾®ä¿¡" in str(c) and "æ·»åŠ " in str(c)) or ("æ·»åŠ å¾®ä¿¡" in str(c))]
        if wechat_cols:
            df_d["S_Wechat"] = _to_1d_numeric(df_d[wechat_cols])
        else:
            df_d["S_Wechat"] = 0

        score_cols = ["è´¨æ£€æ€»åˆ†", "S_60s", "S_Needs", "S_Car", "S_Policy", "S_Wechat", "S_Time"]
        for c in score_cols:
            if c in df_d.columns:
                df_d[c] = pd.to_numeric(df_d[c], errors="coerce")
        if "é‚€çº¦ä¸“å‘˜/ç®¡å®¶" not in df_d.columns:
            df_d["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"] = ""
        df_d = df_d[["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"] + [c for c in score_cols if c in df_d.columns]]

        # ================= C. Store Scores (é—¨åº—è´¨æ£€) =================
        def pick_col_by_keywords(df: pd.DataFrame, must_have_any, must_have_all=None, exclude=None):
            must_have_all = must_have_all or []
            exclude = exclude or []
            for c in df.columns:
                s = str(c)
                if any(k in s for k in must_have_any) and all(k in s for k in must_have_all) and not any(x in s for x in exclude):
                    return c
            return None

        store_name_candidates = [c for c in raw_s.columns if ("é—¨åº—" in str(c)) and ("ID" not in str(c)) and ("ç¼–å·" not in str(c))]
        if store_name_candidates:
            tmp = raw_s[store_name_candidates]
            if isinstance(tmp, pd.Series):
                store_name = tmp.astype(str)
            else:
                store_name = tmp.bfill(axis=1).iloc[:, 0].astype(str)
            store_name = store_name.str.strip()
        else:
            store_name = pd.Series(["" for _ in range(len(raw_s))])

        col_total = pick_col_by_keywords(raw_s, ["è´¨æ£€æ€»åˆ†", "æ€»åˆ†"], exclude=["æ˜¾ç¤º"])
        col_60s = pick_col_by_keywords(raw_s, ["60ç§’", "60 ç§’"], exclude=[])
        col_needs = pick_col_by_keywords(raw_s, ["ç”¨è½¦éœ€æ±‚"], exclude=[])
        col_car = pick_col_by_keywords(raw_s, ["è½¦å‹ä¿¡æ¯"], exclude=[])
        col_policy = pick_col_by_keywords(raw_s, ["æ”¿ç­–"], exclude=[])
        col_time = pick_col_by_keywords(raw_s, ["æ˜ç¡®åˆ°åº—", "åˆ°åº—æ—¶é—´"], exclude=[])
        col_wechat = pick_col_by_keywords(raw_s, ["æ·»åŠ å¾®ä¿¡", "åŠ å¾®ä¿¡", "åŠ å¾®"], exclude=[])

        df_s = pd.DataFrame({"é—¨åº—åç§°": store_name})
        df_s["SR_è´¨æ£€æ€»åˆ†"] = _to_1d_numeric(raw_s[col_total]) if (col_total and col_total in raw_s.columns) else np.nan
        df_s["SR_S_60s"] = _to_1d_numeric(raw_s[col_60s]) if (col_60s and col_60s in raw_s.columns) else np.nan
        df_s["SR_S_Needs"] = _to_1d_numeric(raw_s[col_needs]) if (col_needs and col_needs in raw_s.columns) else np.nan
        df_s["SR_S_Car"] = _to_1d_numeric(raw_s[col_car]) if (col_car and col_car in raw_s.columns) else np.nan
        df_s["SR_S_Policy"] = _to_1d_numeric(raw_s[col_policy]) if (col_policy and col_policy in raw_s.columns) else np.nan
        df_s["SR_S_Wechat"] = _to_1d_numeric(raw_s[col_wechat]) if (col_wechat and col_wechat in raw_s.columns) else np.nan
        df_s["SR_S_Time"] = _to_1d_numeric(raw_s[col_time]) if (col_time and col_time in raw_s.columns) else np.nan

        df_s["é—¨åº—åç§°"] = df_s["é—¨åº—åç§°"].astype(str).str.strip()
        df_s = df_s[df_s["é—¨åº—åç§°"].ne("")].copy()
        df_s = df_s.drop_duplicates(subset=["é—¨åº—åç§°"], keep="first")

        # ================= D. AMS (è·Ÿè¿›æ•°æ®) =================
        cols_config = [
            (["ç®¡å®¶å§“å", "é¡¾é—®å§“å", "é¡¾é—®åç§°", "ç®¡å®¶"], "é‚€çº¦ä¸“å‘˜/ç®¡å®¶", []),
            (["DCCå¹³å‡é€šè¯æ—¶é•¿", "å¹³å‡é€šè¯æ—¶é•¿"], "é€šè¯æ—¶é•¿", []),
            (["DCCæ¥é€šçº¿ç´¢æ•°", "æ¥é€šçº¿ç´¢æ•°"], "conn_num", ["æœªæ¥é€š"]),
            (["DCCå¤–å‘¼çº¿ç´¢æ•°", "å¤–å‘¼çº¿ç´¢æ•°"], "conn_denom", []),
            (["DCCåŠæ—¶å¤„ç†çº¿ç´¢", "åŠæ—¶å¤„ç†çº¿ç´¢"], "timely_num", []),
            (["éœ€å¤–å‘¼çº¿ç´¢æ•°", "éœ€å¤–å‘¼"], "timely_denom", []),
            (["äºŒæ¬¡å¤–å‘¼çº¿ç´¢æ•°", "äºŒæ¬¡å¤–å‘¼"], "call2_num", []),
            (["éœ€å†å‘¼çº¿ç´¢æ•°", "éœ€å†å‘¼"], "call2_denom", []),
            (["DCCä¸‰æ¬¡å¤–å‘¼çš„çº¿ç´¢æ•°", "ä¸‰æ¬¡å¤–å‘¼çº¿ç´¢æ•°", "ä¸‰æ¬¡å¤–å‘¼"], "call3_num", []),
            (["DCCäºŒå‘¼çŠ¶æ€ä¸ºéœ€å†å‘¼çš„çº¿ç´¢æ•°", "äºŒå‘¼çŠ¶æ€ä¸ºéœ€å†å‘¼", "ä¸‰æ¬¡å¤–å‘¼åˆ†æ¯"], "call3_denom", []),
        ]

        target_to_src = {}
        for any_kw, target_name, exclude_kw in cols_config:
            if target_name in target_to_src:
                continue
            found = None
            for col in raw_a.columns:
                s = str(col).strip()
                if any(k in s for k in any_kw) and not any(ex in s for ex in exclude_kw):
                    found = col
                    break
            if found is not None:
                target_to_src[target_name] = found

        rename_map = {src: tgt for tgt, src in target_to_src.items()}
        df_a = raw_a.rename(columns=rename_map)

        all_ams_calc_cols = ["conn_num", "conn_denom", "timely_num", "timely_denom", "call2_num", "call2_denom", "call3_num", "call3_denom"]

        if "é‚€çº¦ä¸“å‘˜/ç®¡å®¶" not in df_a.columns:
            df_a["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"] = ""

        for c in all_ams_calc_cols:
            if c not in df_a.columns:
                df_a[c] = 0
            df_a[c] = _to_1d_numeric(df_a[c])

        if "é€šè¯æ—¶é•¿" not in df_a.columns:
            df_a["é€šè¯æ—¶é•¿"] = 0
        df_a["é€šè¯æ—¶é•¿"] = _to_1d_numeric(df_a["é€šè¯æ—¶é•¿"])

        df_a["å¤–å‘¼æ¥é€šç‡"] = safe_div(df_a, "conn_num", "conn_denom")
        df_a["DCCåŠæ—¶å¤„ç†ç‡"] = safe_div(df_a, "timely_num", "timely_denom")
        df_a["DCCäºŒæ¬¡å¤–å‘¼ç‡"] = safe_div(df_a, "call2_num", "call2_denom")
        df_a["DCCä¸‰æ¬¡å¤–å‘¼ç‡"] = safe_div(df_a, "call3_num", "call3_denom")

        final_ams_cols = ["é‚€çº¦ä¸“å‘˜/ç®¡å®¶", "é€šè¯æ—¶é•¿", "å¤–å‘¼æ¥é€šç‡", "DCCåŠæ—¶å¤„ç†ç‡", "DCCäºŒæ¬¡å¤–å‘¼ç‡", "DCCä¸‰æ¬¡å¤–å‘¼ç‡"] + all_ams_calc_cols
        final_ams_cols = [c for c in final_ams_cols if c in df_a.columns]
        df_a = df_a[final_ams_cols]

        # ================= E. Merge (åˆå¹¶æ•°æ®) =================
        for df in [df_store_data, df_advisor_data, df_d, df_a, df_s]:
            if "é‚€çº¦ä¸“å‘˜/ç®¡å®¶" in df.columns:
                s = _col_as_series(df, "é‚€çº¦ä¸“å‘˜/ç®¡å®¶")
                if s is not None:
                    df["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"] = s.astype(str).str.strip()
            if "é—¨åº—åç§°" in df.columns:
                s2 = _col_as_series(df, "é—¨åº—åç§°")
                if s2 is not None:
                    df["é—¨åº—åç§°"] = s2.astype(str).str.strip()

        full_advisors = pd.merge(df_advisor_data, df_d, on="é‚€çº¦ä¸“å‘˜/ç®¡å®¶", how="left")
        full_advisors = pd.merge(full_advisors, df_a, on="é‚€çº¦ä¸“å‘˜/ç®¡å®¶", how="left")

        cols_to_fill_zero = ["çº¿ç´¢é‡", "åˆ°åº—é‡", "é€šè¯æ—¶é•¿"] + all_ams_calc_cols
        for c in cols_to_fill_zero:
            if c in full_advisors.columns:
                full_advisors[c] = pd.to_numeric(full_advisors[c], errors="coerce").fillna(0)

        ams_agg_dict = {c: "sum" for c in all_ams_calc_cols}
        if "é—¨åº—åç§°" in full_advisors.columns and all(c in full_advisors.columns for c in all_ams_calc_cols):
            store_ams = full_advisors.groupby("é—¨åº—åç§°").agg(ams_agg_dict).reset_index()
        else:
            store_ams = pd.DataFrame(columns=["é—¨åº—åç§°"] + all_ams_calc_cols)

        if not store_ams.empty:
            store_ams["å¤–å‘¼æ¥é€šç‡"] = safe_div(store_ams, "conn_num", "conn_denom")
            store_ams["DCCåŠæ—¶å¤„ç†ç‡"] = safe_div(store_ams, "timely_num", "timely_denom")
            store_ams["DCCäºŒæ¬¡å¤–å‘¼ç‡"] = safe_div(store_ams, "call2_num", "call2_denom")
            store_ams["DCCä¸‰æ¬¡å¤–å‘¼ç‡"] = safe_div(store_ams, "call3_num", "call3_denom")

        full_stores = pd.merge(df_store_data, df_s, on="é—¨åº—åç§°", how="left")
        full_stores = pd.merge(full_stores, store_ams, on="é—¨åº—åç§°", how="left")

        full_stores["è´¨æ£€æ€»åˆ†"] = full_stores.get("SR_è´¨æ£€æ€»åˆ†")
        full_stores["S_60s"] = full_stores.get("SR_S_60s")
        full_stores["S_Needs"] = full_stores.get("SR_S_Needs")
        full_stores["S_Car"] = full_stores.get("SR_S_Car")
        full_stores["S_Policy"] = full_stores.get("SR_S_Policy")
        full_stores["S_Wechat"] = full_stores.get("SR_S_Wechat")
        full_stores["S_Time"] = full_stores.get("SR_S_Time")

        full_stores.drop(columns=[c for c in full_stores.columns if str(c).startswith("SR_")], inplace=True, errors="ignore")
        full_stores.columns = dedupe_columns(full_stores.columns)

        return full_advisors, full_stores

    except Exception as e:
        st.error(f"å¤„ç†å‡ºé”™: {e}")
        import traceback
        st.text(traceback.format_exc())
        return None, None


# ================= 4. ä¾§è¾¹æ é€»è¾‘ =================
with st.sidebar:
    st.header("âš™ï¸ ç®¡ç†é¢æ¿")

    store_rank_path = get_store_rank_path()
    has_data = os.path.exists(PATH_F) and os.path.exists(PATH_D) and os.path.exists(PATH_A) and (store_rank_path is not None)

    if has_data:
        st.success("âœ… æ•°æ®çŠ¶æ€ï¼šå·²å°±ç»ª")
    else:
        st.warning("âš ï¸ æš‚æ— æ•°æ®")
    st.markdown("---")

    with st.expander("ğŸ” æ›´æ–°æ•°æ® (ä»…é™ç®¡ç†å‘˜)"):
        pwd = st.text_input("è¾“å…¥ç®¡ç†å‘˜å¯†ç ", type="password")
        if pwd == ADMIN_PASSWORD:
            st.info("ğŸ”“ è¯·ä¸Šä¼ æ–°æ–‡ä»¶ï¼š")
            new_f = st.file_uploader("1. æ¼æ–—æŒ‡æ ‡è¡¨", type=["xlsx", "csv"], key="up_f")
            new_d = st.file_uploader("2. é¡¾é—®è´¨æ£€è¡¨", type=["xlsx", "csv"], key="up_d")
            new_a = st.file_uploader("3. AMSè·Ÿè¿›è¡¨", type=["xlsx", "csv"], key="up_a")
            new_s = st.file_uploader("4. é—¨åº—æ’åè¡¨", type=["xlsx", "csv"], key="up_s")

            new_m = st.file_uploader("5. ä»£ç†å•†åç§°å½’å±(åŒºåŸŸç»ç†/çœä»½/åŸå¸‚/é—¨åº—)", type=["xlsx"], key="up_m")

            if st.button("ğŸš€ ç¡®è®¤æ›´æ–°æ•°æ®"):
                if new_f and new_d and new_a and new_s:
                    with st.spinner("æ­£åœ¨ä¿å­˜æ•°æ®..."):
                        save_uploaded_file(new_f, PATH_F)
                        save_uploaded_file(new_d, PATH_D)
                        save_uploaded_file(new_a, PATH_A)

                        if str(new_s.name).lower().endswith(".xlsx"):
                            if os.path.exists(PATH_S_CSV):
                                try:
                                    os.remove(PATH_S_CSV)
                                except Exception:
                                    pass
                            save_uploaded_file(new_s, PATH_S_XLSX)
                        else:
                            if os.path.exists(PATH_S_XLSX):
                                try:
                                    os.remove(PATH_S_XLSX)
                                except Exception:
                                    pass
                            save_uploaded_file(new_s, PATH_S_CSV)

                        if new_m is not None:
                            save_uploaded_file(new_m, PATH_M)

                        try:
                            with open(LAST_UPDATE_FILE, "w", encoding="utf-8") as f:
                                f.write(datetime.now().isoformat(timespec="seconds"))
                        except Exception:
                            pass

                    st.success("æ›´æ–°å®Œæˆï¼Œæ­£åœ¨åˆ·æ–°...")
                    st.rerun()

                elif new_m is not None:
                    with st.spinner("æ­£åœ¨ä¿å­˜å½’å±è¡¨..."):
                        save_uploaded_file(new_m, PATH_M)
                    st.success("å½’å±è¡¨æ›´æ–°å®Œæˆï¼Œæ­£åœ¨åˆ·æ–°...")
                    st.rerun()

                else:
                    st.error("è¯·ä¼ é½ 4 ä¸ªæ–‡ä»¶ï¼ˆæˆ–è‡³å°‘å•ç‹¬ä¸Šä¼ ç¬¬5ä¸ªå½’å±è¡¨ï¼‰")


# ================= 5. ç•Œé¢æ¸²æŸ“ =================
store_rank_path = get_store_rank_path()
has_data = os.path.exists(PATH_F) and os.path.exists(PATH_D) and os.path.exists(PATH_A) and (store_rank_path is not None)

if has_data:
    df_advisors, df_stores = process_data(PATH_F, PATH_D, PATH_A, store_rank_path)

    if df_advisors is not None:
        col_header, col_update, col_filter = st.columns([2.4, 1.2, 1])
        with col_header:
            st.title("Audi | DCC æ•ˆèƒ½çœ‹æ¿")

        with col_update:
            upd = get_data_update_time(store_rank_path)
            upd_text = upd.strftime("%Y-%m-%d %H:%M") if upd else "æš‚æ— "
            st.markdown(
                f"""
                <div style='text-align:right; padding-top: 12px;'>
                  <span style='display:inline-block; padding:6px 10px; border-radius:999px; border:1px solid rgba(49, 51, 63, 0.18); background: rgba(49, 51, 63, 0.06); font-size: 12px;'>
                    ğŸ•’ æ•°æ®æ›´æ–°æ—¶é—´ï¼š<b>{upd_text}</b>
                  </span>
                </div>
                """,
                unsafe_allow_html=True,
            )

        filter_badge = "å…¨ä½“"

        with col_filter:
            if df_stores is not None and not df_stores.empty and "é—¨åº—åç§°" in df_stores.columns:
                all_stores = sorted(list(df_stores["é—¨åº—åç§°"].dropna().astype(str).str.strip().unique()))
            else:
                all_stores = sorted(list(df_advisors.get("é—¨åº—åç§°", pd.Series(dtype=str)).dropna().astype(str).str.strip().unique()))

            # âœ… è‡ªæ£€è¡Œï¼ˆåªå±•ç¤ºï¼Œä¸å½±å“åŠŸèƒ½ï¼‰
            map_path = _resolve_store_map_path()
            map_exists = bool(map_path and os.path.exists(map_path))
            map_mtime = datetime.fromtimestamp(os.path.getmtime(map_path)).strftime("%Y-%m-%d %H:%M:%S") if map_exists else "â€”"
            st.caption(f"ğŸ§­ å½’å±è¡¨è‡ªæ£€ï¼š{'âœ…å·²æ£€æµ‹åˆ°' if map_exists else 'âŒæœªæ£€æµ‹åˆ°'} ï½œ è·¯å¾„ï¼š{map_path or 'æ— '} ï½œ ä¿®æ”¹æ—¶é—´ï¼š{map_mtime}")

            store_map = get_store_map_df()
            allowed_stores = all_stores[:]

            if store_map is None:
                st.warning("æœªåŠ è½½é—¨åº—å½’å±è¡¨ï¼ˆç¬¬5é¡¹ï¼‰æˆ–åˆ—åä¸åŒ¹é…ï¼ˆéœ€ï¼šåŒºåŸŸç»ç†/çœä»½/åŸå¸‚/é—¨åº—åç§°ï¼‰ã€‚å°†å›é€€åˆ°ã€é—¨åº—ä¸‹æ‹‰ã€‘æ¨¡å¼ã€‚")
                store_options = ["å…¨éƒ¨"] + all_stores
                selected_store = st.selectbox("ğŸ­ åˆ‡æ¢é—¨åº—è§†å›¾", store_options)
            else:
                mgr_opts = ["å…¨ä½“"] + sorted(store_map["åŒºåŸŸç»ç†"].dropna().astype(str).str.strip().unique().tolist())
                sel_mgr = st.selectbox("åŒºåŸŸç»ç†", mgr_opts, key="sel_mgr")

                tmp = store_map if sel_mgr == "å…¨ä½“" else store_map[store_map["åŒºåŸŸç»ç†"] == sel_mgr]

                prov_opts = ["å…¨ä½“"] + sorted(tmp["çœä»½"].dropna().astype(str).str.strip().unique().tolist())
                sel_prov = st.selectbox("çœä»½", prov_opts, key="sel_prov")

                tmp2 = tmp if sel_prov == "å…¨ä½“" else tmp[tmp["çœä»½"] == sel_prov]

                city_opts = ["å…¨ä½“"] + sorted(tmp2["åŸå¸‚"].dropna().astype(str).str.strip().unique().tolist())
                sel_city = st.selectbox("åŸå¸‚", city_opts, key="sel_city")

                tmp3 = tmp2 if sel_city == "å…¨ä½“" else tmp2[tmp2["åŸå¸‚"] == sel_city]

                store_opts = ["å…¨ä½“"] + sorted([s for s in tmp3["é—¨åº—åç§°"].dropna().astype(str).str.strip().unique().tolist() if s in set(all_stores)])
                sel_store = st.selectbox("é—¨åº—åç§°", store_opts, key="sel_store")

                mm = store_map.copy()
                if sel_mgr != "å…¨ä½“":
                    mm = mm[mm["åŒºåŸŸç»ç†"] == sel_mgr]
                if sel_prov != "å…¨ä½“":
                    mm = mm[mm["çœä»½"] == sel_prov]
                if sel_city != "å…¨ä½“":
                    mm = mm[mm["åŸå¸‚"] == sel_city]
                if sel_store != "å…¨ä½“":
                    mm = mm[mm["é—¨åº—åç§°"] == sel_store]

                allowed_stores = sorted([s for s in mm["é—¨åº—åç§°"].dropna().astype(str).str.strip().unique().tolist() if s in set(all_stores)])

                parts = []
                if sel_mgr != "å…¨ä½“":
                    parts.append(sel_mgr)
                if sel_prov != "å…¨ä½“":
                    parts.append(sel_prov)
                if sel_city != "å…¨ä½“":
                    parts.append(sel_city)
                if sel_store != "å…¨ä½“":
                    parts.append(sel_store)
                filter_badge = " / ".join(parts) if parts else "å…¨ä½“"
                st.caption(f"å½“å‰ç­›é€‰ï¼š{filter_badge}")

                selected_store = "å…¨éƒ¨" if sel_store == "å…¨ä½“" else sel_store

            if allowed_stores is not None:
                if df_stores is not None and not df_stores.empty and "é—¨åº—åç§°" in df_stores.columns:
                    df_stores = df_stores[df_stores["é—¨åº—åç§°"].astype(str).str.strip().isin(allowed_stores)].copy()
                if df_advisors is not None and not df_advisors.empty and "é—¨åº—åç§°" in df_advisors.columns:
                    df_advisors = df_advisors[df_advisors["é—¨åº—åç§°"].astype(str).str.strip().isin(allowed_stores)].copy()

        if selected_store == "å…¨éƒ¨":
            current_df = df_stores.copy() if df_stores is not None else pd.DataFrame()
            current_df["åç§°"] = current_df.get("é—¨åº—åç§°", "")
            rank_title = f"ğŸ† {filter_badge} é—¨åº—æ’å"
            kpi_leads = current_df.get("çº¿ç´¢é‡", pd.Series(dtype=float)).sum()
            kpi_visits = current_df.get("åˆ°åº—é‡", pd.Series(dtype=float)).sum()
            kpi_rate = kpi_visits / kpi_leads if kpi_leads > 0 else 0
            kpi_score = current_df.get("è´¨æ£€æ€»åˆ†", pd.Series(dtype=float)).mean() if "è´¨æ£€æ€»åˆ†" in current_df.columns else 0
        else:
            current_df = df_advisors[df_advisors.get("é—¨åº—åç§°", "") == selected_store].copy()
            current_df["åç§°"] = current_df.get("é‚€çº¦ä¸“å‘˜/ç®¡å®¶", "")
            rank_title = f"ğŸ‘¤ {selected_store} - é¡¾é—®æ’å"
            kpi_leads = current_df.get("çº¿ç´¢é‡", pd.Series(dtype=float)).sum()
            kpi_visits = current_df.get("åˆ°åº—é‡", pd.Series(dtype=float)).sum()
            kpi_rate = kpi_visits / kpi_leads if kpi_leads > 0 else 0
            kpi_score = current_df.get("è´¨æ£€æ€»åˆ†", pd.Series(dtype=float)).mean() if "è´¨æ£€æ€»åˆ†" in current_df.columns else 0

        # 1. Result
        st.subheader("1ï¸âƒ£ ç»“æœæ¦‚è§ˆ (Result)")
        k1, k2, k3, k4 = st.columns(4)
        k1.metric("æ€»æœ‰æ•ˆçº¿ç´¢", f"{int(kpi_leads):,}")
        k2.metric("æ€»å®é™…åˆ°åº—", f"{int(kpi_visits):,}")
        k3.metric("çº¿ç´¢åˆ°åº—ç‡", f"{kpi_rate:.1%}")
        k4.metric("å¹³å‡è´¨æ£€æ€»åˆ†", f"{kpi_score:.1f}")

        # 2. Process
        st.markdown("---")
        st.subheader("2ï¸âƒ£ DCC å¤–å‘¼è¿‡ç¨‹ç›‘æ§ (Process)")

        def calc_kpi_rate(df, num, denom):
            if num not in df.columns or denom not in df.columns:
                return 0
            total_num = pd.to_numeric(df[num], errors="coerce").fillna(0).sum()
            total_denom = pd.to_numeric(df[denom], errors="coerce").fillna(0).sum()
            return total_num / total_denom if total_denom > 0 else 0

        p1, p2, p3, p4 = st.columns(4)
        avg_conn = calc_kpi_rate(current_df, "conn_num", "conn_denom")
        avg_timely = calc_kpi_rate(current_df, "timely_num", "timely_denom")
        avg_call2 = calc_kpi_rate(current_df, "call2_num", "call2_denom")
        avg_call3 = calc_kpi_rate(current_df, "call3_num", "call3_denom")

        p1.metric("ğŸ“ å¤–å‘¼æ¥é€šç‡", f"{avg_conn:.1%}")
        p2.metric("âš¡ DCCåŠæ—¶å¤„ç†ç‡", f"{avg_timely:.1%}")
        p3.metric("ğŸ”„ äºŒæ¬¡å¤–å‘¼ç‡", f"{avg_call2:.1%}")
        p4.metric("ğŸ” ä¸‰æ¬¡å¤–å‘¼ç‡", f"{avg_call3:.1%}")
        st.caption("æ³¨ï¼šä»¥ä¸Šä¸ºåŠ æƒå¹³å‡å€¼ï¼ˆsum/sumï¼‰")

        plot_df_vis = current_df.copy()
        if "è´¨æ£€æ€»åˆ†" in plot_df_vis.columns:
            plot_df_vis["è´¨æ£€æ€»åˆ†_æ˜¾ç¤º"] = plot_df_vis["è´¨æ£€æ€»åˆ†"].fillna(0)
        else:
            plot_df_vis["è´¨æ£€æ€»åˆ†_æ˜¾ç¤º"] = 0

        c_proc_1, c_proc_2 = st.columns(2)
        with c_proc_1:
            st.markdown("#### ğŸ•µï¸ å¼‚å¸¸ä¾¦æµ‹ï¼šå¤–å‘¼æ¥é€šç‡ vs 60ç§’é€šè¯å æ¯”")
            st.info("ğŸ’¡ å³ä¸‹è§’ï¼ˆæ¥é€šç‡é«˜ä½†60ç§’å æ¯”ä½ï¼‰é€šå¸¸ä»£è¡¨ï¼šå¯èƒ½å­˜åœ¨è¯æœ¯å¼±/äººä¸ºå‹æ—¶é•¿ã€‚")

            if "S_60s" in plot_df_vis.columns and "å¤–å‘¼æ¥é€šç‡" in plot_df_vis.columns:
                fig_p1 = px.scatter(
                    plot_df_vis,
                    x="å¤–å‘¼æ¥é€šç‡",
                    y="S_60s",
                    size="çº¿ç´¢é‡" if "çº¿ç´¢é‡" in plot_df_vis.columns else None,
                    color="è´¨æ£€æ€»åˆ†_æ˜¾ç¤º",
                    hover_name="åç§°",
                    labels={"å¤–å‘¼æ¥é€šç‡": "å¤–å‘¼æ¥é€šç‡", "S_60s": "60ç§’é€šè¯å æ¯”å¾—åˆ†"},
                    color_continuous_scale="RdYlGn",
                    height=350,
                )
                fig_p1.add_vline(x=avg_conn, line_dash="dash", line_color="gray")
                if "S_60s" in plot_df_vis.columns:
                    fig_p1.add_hline(
                        y=pd.to_numeric(plot_df_vis["S_60s"], errors="coerce").fillna(0).mean(),
                        line_dash="dash",
                        line_color="gray",
                    )
                fig_p1.update_layout(xaxis=dict(tickformat=".0%"))
                st.plotly_chart(fig_p1, use_container_width=True)
            else:
                st.warning("ç¼ºå°‘å¤–å‘¼æ¥é€šç‡æˆ–60ç§’é€šè¯æ•°æ®ï¼Œæ— æ³•ç»˜å›¾")

        with c_proc_2:
            st.markdown("#### ğŸ”— å½’å› åˆ†æï¼šè¿‡ç¨‹æŒ‡æ ‡ vs çº¿ç´¢é¦–é‚€åˆ°åº—ç‡")
            st.info("ğŸ’¡ è§‚å¯Ÿå¤–å‘¼åŠæ—¶æ€§ä¸é‚€çº¦åˆ°åº—ç‡ç›¸å…³æ€§ã€‚")

            x_axis_choice = st.radio("é€‰æ‹©æ¨ªè½´æŒ‡æ ‡ï¼š", ["DCCåŠæ—¶å¤„ç†ç‡", "DCCäºŒæ¬¡å¤–å‘¼ç‡", "DCCä¸‰æ¬¡å¤–å‘¼ç‡"], horizontal=True)
            plot_df_corr = plot_df_vis.copy()
            plot_df_corr["çº¿ç´¢åˆ°åº—ç‡_æ˜¾ç¤º"] = pd.to_numeric(plot_df_corr.get("çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼", 0), errors="coerce").fillna(0).clip(0, 1)

            if x_axis_choice in plot_df_corr.columns:
                plot_df_corr[x_axis_choice] = pd.to_numeric(plot_df_corr[x_axis_choice], errors="coerce").fillna(0).clip(0, 1)

                fig_p2 = px.scatter(
                    plot_df_corr,
                    x=x_axis_choice,
                    y="çº¿ç´¢åˆ°åº—ç‡_æ˜¾ç¤º",
                    size="çº¿ç´¢é‡" if "çº¿ç´¢é‡" in plot_df_corr.columns else None,
                    color="è´¨æ£€æ€»åˆ†_æ˜¾ç¤º",
                    hover_name="åç§°",
                    labels={x_axis_choice: x_axis_choice, "çº¿ç´¢åˆ°åº—ç‡_æ˜¾ç¤º": "çº¿ç´¢åˆ°åº—ç‡"},
                    color_continuous_scale="Blues",
                    height=300,
                )

                fig_p2.update_xaxes(range=[0, 1.02], tickformat=".0%", tick0=0, dtick=0.2)
                fig_p2.update_yaxes(tickformat=".1%")
                fig_p2.update_traces(cliponaxis=False)
                fig_p2.update_layout(margin=dict(r=70))

                if "çº¿ç´¢é‡" in plot_df_corr.columns:
                    fig_p2.update_traces(
                        customdata=np.stack(
                            (
                                pd.to_numeric(plot_df_corr["çº¿ç´¢é‡"], errors="coerce").fillna(0),
                                plot_df_corr[x_axis_choice],
                                plot_df_corr["çº¿ç´¢åˆ°åº—ç‡_æ˜¾ç¤º"],
                                pd.to_numeric(plot_df_corr["è´¨æ£€æ€»åˆ†_æ˜¾ç¤º"], errors="coerce").fillna(0),
                            ),
                            axis=-1,
                        ),
                        cliponaxis=False,
                        hovertemplate=(
                            "<b>%{hovertext}</b><br><br>"
                            "çº¿ç´¢é‡: %{customdata[0]:,.0f}<br>"
                            + f"{x_axis_choice}: %{{customdata[1]:.1%}}<br>"
                            "çº¿ç´¢åˆ°åº—ç‡: %{customdata[2]:.1%}<br>"
                            "è´¨æ£€æ€»åˆ†: %{customdata[3]:.1f}<br>"
                            "<extra></extra>"
                        ),
                    )
                else:
                    fig_p2.update_traces(
                        customdata=np.stack(
                            (
                                plot_df_corr[x_axis_choice],
                                plot_df_corr["çº¿ç´¢åˆ°åº—ç‡_æ˜¾ç¤º"],
                                pd.to_numeric(plot_df_corr["è´¨æ£€æ€»åˆ†_æ˜¾ç¤º"], errors="coerce").fillna(0),
                            ),
                            axis=-1,
                        ),
                        hovertemplate=(
                            "<b>%{hovertext}</b><br><br>"
                            + f"{x_axis_choice}: %{{customdata[0]:.1%}}<br>"
                            "çº¿ç´¢åˆ°åº—ç‡: %{customdata[1]:.1%}<br>"
                            "è´¨æ£€æ€»åˆ†: %{customdata[2]:.1f}<br>"
                            "<extra></extra>"
                        ),
                    )

                st.plotly_chart(fig_p2, use_container_width=True)
            else:
                st.warning("å½“å‰è§†å›¾ç¼ºå°‘æ‰€é€‰è¿‡ç¨‹æŒ‡æ ‡åˆ—ï¼Œæ— æ³•ç»˜å›¾")

        st.markdown("---")

        # 3. Rank & Diagnosis
        c_left, c_right = st.columns([1, 2])
        with c_left:
            st.markdown(f"### ğŸ† {rank_title}")
            if all(c in current_df.columns for c in ["åç§°", "çº¿ç´¢åˆ°åº—ç‡", "çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼"]):
                rank_df = current_df[["åç§°", "çº¿ç´¢åˆ°åº—ç‡", "çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼"]].copy()
                rank_df["è´¨æ£€æ€»åˆ†"] = current_df["è´¨æ£€æ€»åˆ†"] if "è´¨æ£€æ€»åˆ†" in current_df.columns else 0
                rank_df["Sort_Score"] = pd.to_numeric(rank_df["çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼"], errors="coerce").fillna(-1)
                rank_df = rank_df.sort_values("Sort_Score", ascending=False).head(15)
                display_df = rank_df[["åç§°", "çº¿ç´¢åˆ°åº—ç‡", "è´¨æ£€æ€»åˆ†"]]

                st.dataframe(
                    display_df,
                    hide_index=True,
                    use_container_width=True,
                    height=400,
                    column_config={
                        "åç§°": st.column_config.TextColumn("åç§°"),
                        "çº¿ç´¢åˆ°åº—ç‡": st.column_config.TextColumn("çº¿ç´¢åˆ°åº—ç‡"),
                        "è´¨æ£€æ€»åˆ†": st.column_config.NumberColumn("è´¨æ£€æ€»åˆ†", format="%.1f"),
                    },
                )
            else:
                st.warning("å½“å‰è§†å›¾ç¼ºå°‘æ’è¡Œå¿…éœ€åˆ—")

        with c_right:
            st.markdown("### ğŸ’¡ è¯æœ¯è´¨é‡ vs è½¬åŒ–ç»“æœ")
            if "S_Time" in plot_df_vis.columns:
                plot_df = plot_df_vis.copy()
                plot_df["è½¬åŒ–ç‡%"] = pd.to_numeric(plot_df.get("çº¿ç´¢åˆ°åº—ç‡_æ•°å€¼", 0), errors="coerce").fillna(0) * 100
                fig = px.scatter(
                    plot_df,
                    x="S_Time",
                    y="è½¬åŒ–ç‡%",
                    size="çº¿ç´¢é‡" if "çº¿ç´¢é‡" in plot_df.columns else None,
                    color="è´¨æ£€æ€»åˆ†_æ˜¾ç¤º",
                    hover_name="åç§°",
                    labels={"S_Time": "æ˜ç¡®åˆ°åº—æ—¶é—´å¾—åˆ†", "è½¬åŒ–ç‡%": "çº¿ç´¢åˆ°åº—ç‡(%)"},
                    color_continuous_scale="Reds",
                    height=400,
                )

                s60 = pd.to_numeric(plot_df.get("S_60s", 0), errors="coerce").fillna(0)
                total = pd.to_numeric(plot_df.get("è´¨æ£€æ€»åˆ†", 0), errors="coerce").fillna(0)
                leads = pd.to_numeric(plot_df.get("çº¿ç´¢é‡", 0), errors="coerce").fillna(0)
                fig.update_traces(
                    customdata=np.stack((leads, s60, total), axis=-1),
                    hovertemplate=(
                        "<b>%{hovertext}</b><br><br>"
                        "æ˜ç¡®åˆ°åº—æ—¶é—´å¾—åˆ†: %{x:.1f}<br>"
                        "çº¿ç´¢åˆ°åº—ç‡: %{y:.1f}%<br>"
                        "çº¿ç´¢é‡: %{customdata[0]:,.0f}<br>"
                        "60ç§’é€šè¯å æ¯”å¾—åˆ†: %{customdata[1]:.1f}<br>"
                        "è´¨æ£€æ€»åˆ†: %{customdata[2]:.1f}<br>"
                        "<extra></extra>"
                    ),
                )

                if not plot_df.empty:
                    fig.add_vline(x=pd.to_numeric(plot_df["S_Time"], errors="coerce").fillna(0).mean(), line_dash="dash", line_color="gray")
                    fig.add_hline(y=kpi_rate * 100, line_dash="dash", line_color="gray")

                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning("ç¼ºå°‘â€œæ˜ç¡®åˆ°åº—æ—¶é—´â€æ•°æ®ï¼Œæ— æ³•ç»˜å›¾")

        st.markdown("---")

        # 4. æ·±åº¦è¯Šæ–­
        with st.container():
            st.markdown("### ğŸ•µï¸â€â™€ï¸ é‚€çº¦ä¸“å‘˜/ç®¡å®¶æ·±åº¦è¯Šæ–­")
            if selected_store == "å…¨éƒ¨":
                st.info("ğŸ’¡ è¯·å…ˆé€‰æ‹©å…·ä½“ã€é—¨åº—åç§°ã€‘ï¼ˆå››çº§è”åŠ¨ä¸­é€‰åˆ°å…·ä½“é—¨åº—ï¼‰ï¼ŒæŸ¥çœ‹è¯¥é—¨åº—ä¸‹çš„é¡¾é—®è¯¦ç»†è¯Šæ–­ã€‚")
            else:
                diag_df = current_df.copy()
                if "çº¿ç´¢é‡" in diag_df.columns:
                    diag_df = diag_df[pd.to_numeric(diag_df["çº¿ç´¢é‡"], errors="coerce").fillna(0) > 0].copy()

                diag_list = sorted(diag_df["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"].dropna().astype(str).unique()) if "é‚€çº¦ä¸“å‘˜/ç®¡å®¶" in diag_df.columns else []
                if diag_list:
                    selected_person = st.selectbox("ğŸ” é€‰æ‹©è¯¥åº—é‚€çº¦ä¸“å‘˜/ç®¡å®¶ï¼š", diag_list)
                    p_row = df_advisors[df_advisors["é‚€çº¦ä¸“å‘˜/ç®¡å®¶"] == selected_person]
                    if p_row.empty:
                        st.warning("æ‰¾ä¸åˆ°è¯¥äººå‘˜æ˜ç»†")
                    else:
                        p = p_row.iloc[0]
                        d1, d2, d3 = st.columns([1, 1, 1.2])

                        with d1:
                            st.caption("è½¬åŒ–æ¼æ–— (RESULT)")
                            leads = float(pd.to_numeric(p.get("çº¿ç´¢é‡", 0), errors="coerce") or 0)
                            visits = float(pd.to_numeric(p.get("åˆ°åº—é‡", 0), errors="coerce") or 0)
                            fig_f = go.Figure(
                                go.Funnel(
                                    y=["çº¿ç´¢é‡", "åˆ°åº—é‡"],
                                    x=[leads, visits],
                                    textinfo="value+percent initial",
                                    marker={"color": ["#d9d9d9", "#bb0a30"]},
                                )
                            )
                            fig_f.update_layout(showlegend=False, height=180, margin=dict(t=0, b=0, l=0, r=0))
                            st.plotly_chart(fig_f, use_container_width=True)

                            st.metric("çº¿ç´¢åˆ°åº—ç‡", p.get("çº¿ç´¢åˆ°åº—ç‡", "0.0%"))
                            st.caption(f"å¹³å‡é€šè¯æ—¶é•¿: {float(pd.to_numeric(p.get('é€šè¯æ—¶é•¿', 0), errors='coerce') or 0):.1f} ç§’")

                        has_score = ("è´¨æ£€æ€»åˆ†" in p.index) and (not pd.isna(p.get("è´¨æ£€æ€»åˆ†")))

                        with d2:
                            st.caption("è´¨æ£€å¾—åˆ†è¯¦æƒ… (QUALITY)")
                            if has_score:
                                metrics = {
                                    "æ˜ç¡®åˆ°åº—æ—¶é—´": p.get("S_Time", np.nan),
                                    "60ç§’é€šè¯å æ¯”": p.get("S_60s", np.nan),
                                    "ç”¨è½¦éœ€æ±‚": p.get("S_Needs", np.nan),
                                    "è½¦å‹ä¿¡æ¯ä»‹ç»": p.get("S_Car", np.nan),
                                    "æ”¿ç­–ç›¸å…³è¯æœ¯": p.get("S_Policy", np.nan),
                                    "æ·»åŠ å¾®ä¿¡": p.get("S_Wechat", np.nan),
                                }
                                for k, v in metrics.items():
                                    val = 0 if pd.isna(v) else float(v)
                                    c_a, c_b = st.columns([3, 1])
                                    c_a.progress(min(val / 100, 1.0))
                                    c_b.write(f"{val:.1f}")
                                    st.caption(k)
                            else:
                                st.warning("æš‚æ— è´¨æ£€æ•°æ®")

                        with d3:
                            if has_score:
                                st.error("ğŸ¤– AI æ™ºèƒ½è¯Šæ–­å»ºè®®")
                                val_60s = 0 if pd.isna(p.get("S_60s", np.nan)) else float(p.get("S_60s"))

                                other_kpis = {
                                    "æ˜ç¡®åˆ°åº—": (p.get("S_Time", np.nan), "å»ºè®®ä½¿ç”¨äºŒé€‰ä¸€æ³•é”å®šæ—¶é—´ã€‚"),
                                    "æ·»åŠ å¾®ä¿¡": (p.get("S_Wechat", np.nan), "å»ºè®®ä»¥å‘å®šä½/èµ„æ–™ä¸ºç”±åŠ å¾®ã€‚"),
                                    "ç”¨è½¦éœ€æ±‚": (p.get("S_Needs", np.nan), "éœ€åŠ å¼ºéœ€æ±‚æŒ–æ˜ï¼Œè‡³å°‘é—®æ¸…åœºæ™¯/é¢„ç®—/å®¶åº­ç»“æ„ã€‚"),
                                    "è½¦å‹ä¿¡æ¯": (p.get("S_Car", np.nan), "éœ€æå‡äº§å“è®²è§£é“¾è·¯ï¼Œå…ˆè®²1-2ä¸ªå¼ºå–ç‚¹ã€‚"),
                                    "æ”¿ç­–ç›¸å…³": (p.get("S_Policy", np.nan), "éœ€å‡†ç¡®ä¼ è¾¾æ”¿ç­–ï¼Œå¹¶ç”¨æˆªæ­¢æ—¶é—´æ¨åŠ¨å†³ç­–ã€‚"),
                                }

                                issues_list = []
                                is_failing = False

                                if val_60s < 60:
                                    issues_list.append(f"ğŸŸ  **60ç§’å æ¯” (å¾—åˆ†{val_60s:.1f})**\nå¼€åœºå…ˆæŠ›åˆ©ç›Šç‚¹ + æ˜ç¡®ä¸‹ä¸€æ­¥åŠ¨ä½œã€‚")
                                    is_failing = True

                                cleaned_others = {}
                                for k, (v, advice) in other_kpis.items():
                                    score = 0 if pd.isna(v) else float(v)
                                    cleaned_others[k] = (score, advice)
                                    if score < 80:
                                        issues_list.append(f"ğŸ”´ **{k} (å¾—åˆ†{score:.1f})**\n{advice}")
                                        is_failing = True

                                if is_failing:
                                    for item in issues_list:
                                        st.markdown(item)
                                    st.warning("âš ï¸ å­˜åœ¨æ˜æ˜¾çŸ­æ¿ï¼Œè¯·é‡ç‚¹è¾…å¯¼ã€‚")
                                else:
                                    all_above_85 = all(score >= 85 for score, _ in cleaned_others.values())
                                    if all_above_85:
                                        st.success("ğŸŒŸ å„é¡¹æŒ‡æ ‡è¡¨ç°ä¼˜ç§€ï¼")
                                    else:
                                        st.info("âœ… å„é¡¹æŒ‡æ ‡åˆæ ¼ï¼Œä½†ä»æœ‰æå‡ç©ºé—´ã€‚")
                            else:
                                st.info("æš‚æ— æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆè¯Šæ–­å»ºè®®ã€‚")
                else:
                    st.warning("è¯¥é—¨åº—ä¸‹æš‚æ— æ•°æ®ã€‚")
else:
    st.info("ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ Audi æ•ˆèƒ½çœ‹æ¿ï¼")
    st.warning("ğŸ‘‰ ç›®å‰æš‚æ— æ•°æ®ã€‚è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ å±•å¼€ã€æ›´æ–°æ•°æ®ã€‘ï¼Œè¾“å…¥ç®¡ç†å‘˜å¯†ç å¹¶ä¸Šä¼ æ‰€æœ‰ **4** ä¸ªæ•°æ®æ–‡ä»¶ï¼ˆå½’å±è¡¨ç¬¬5é¡¹å¯é€‰ï¼‰ã€‚")
